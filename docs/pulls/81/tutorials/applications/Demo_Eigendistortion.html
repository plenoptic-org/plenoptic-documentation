

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reproducing Berardino et al., 2017 (Eigendistortions) &mdash; plenoptic 1.2.1.dev138 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f712bd2b"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=35a8b989"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Synthesis object design" href="../../synthesis.html" />
    <link rel="prev" title="Reproducing Wang and Simoncelli, 2008 (MAD Competition)" href="09_Original_MAD.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citation.html">Citation Guide and Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Reproducing Berardino et al., 2017 (Eigendistortions)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/applications/Demo_Eigendistortion.nblink.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><strong>Run notebook online with Binder:</strong><a class="reference external" href="https://mybinder.org/v2/gh/plenoptic-org/plenoptic/1.1.0?filepath=examples/Demo_Eigendistortion.ipynb"><img alt="Binder" src="http://mybinder.org/badge_logo.svg" /></a></p>
<section id="Reproducing-Berardino-et-al.,-2017-(Eigendistortions)">
<h1>Reproducing Berardino et al., 2017 (Eigendistortions)<a class="headerlink" href="#Reproducing-Berardino-et-al.,-2017-(Eigendistortions)" title="Link to this heading"></a></h1>
<p>Author: Lyndon Duong, Jan 2021</p>
<p>In this demo, we will be reproducing eigendistortions first presented in <a class="reference external" href="https://arxiv.org/abs/1710.02266">Berardino et al 2017</a>. We’ll be using a Front End model of the human visual system (called “On-Off” in the paper), as well as an early layer of VGG16. The Front End model is a simple convolutional neural network with a normalization nonlinearity, loosely based on biological retinal/geniculate circuitry.</p>
<p><img alt="Front-end model" src="../../_images/front_end_model.png" /></p>
<p>This signal-flow diagram shows an input being decomposed into two channels, with each being luminance and contrast normalized, and ending with a ReLu.</p>
<section id="What-do-eigendistortions-tell-us?">
<h2>What do eigendistortions tell us?<a class="headerlink" href="#What-do-eigendistortions-tell-us?" title="Link to this heading"></a></h2>
<p>Our perception is influenced by our internal representation (neural responses) of the external world. Eigendistortions are rank-ordered directions in image space, along which a model’s responses are more sensitive. <code class="docutils literal notranslate"><span class="pre">Plenoptic</span></code>’s <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> object provides an easy way to synthesize eigendistortions for any PyTorch model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">plenoptic.simulate.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">OnOff</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">plenoptic.synthesize</span><span class="w"> </span><span class="kn">import</span> <span class="n">Eigendistortion</span>

<span class="c1"># this notebook uses torchvision, which is an optional dependency.</span>
<span class="c1"># if this fails, install torchvision in your plenoptic environment</span>
<span class="c1"># and restart the notebook kernel.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">feature_extraction</span>
<span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span>
        <span class="s2">&quot;optional dependency torchvision not found!&quot;</span>
        <span class="s2">&quot; please install it in your plenoptic environment &quot;</span>
        <span class="s2">&quot;and restart the notebook kernel&quot;</span>
    <span class="p">)</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;device: &quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.
  return self._float_to_str(self.smallest_subnormal)
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.
  return self._float_to_str(self.smallest_subnormal)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
device:  cuda
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_iter_frontend</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">max_iter_vgg</span> <span class="o">=</span> <span class="mi">5000</span>
</pre></div>
</div>
</div>
</section>
<section id="Input-preprocessing">
<h2>Input preprocessing<a class="headerlink" href="#Input-preprocessing" title="Link to this heading"></a></h2>
<p>Let’s load the parrot image used in the paper and display it:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># crop the image to be square:</span>
<span class="n">image_tensor</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">parrot</span><span class="p">(</span><span class="n">as_gray</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># reduce size of image if we&#39;re on CPU, otherwise this will take too long</span>
<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
    <span class="n">sz</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">zoom</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">/</span> <span class="mi">64</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">sz</span> <span class="o">=</span> <span class="mi">254</span>
    <span class="n">zoom</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">image_tensor</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">center_crop</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">,</span> <span class="n">sz</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch image shape:&quot;</span><span class="p">,</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Torch image shape: torch.Size([1, 1, 254, 254])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_4_1.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_4_1.png" />
</div>
</div>
<p>Since the Front-end OnOff model only has two channel outputs, we can easily visualize the feature maps. We’ll apply a circular mask to this model’s inputs to avoid edge artifacts in the synthesis method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mdl_f</span> <span class="o">=</span> <span class="n">OnOff</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">31</span><span class="p">,</span> <span class="mi">31</span><span class="p">),</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">apply_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">remove_grad</span><span class="p">(</span><span class="n">mdl_f</span><span class="p">)</span>
<span class="n">mdl_f</span> <span class="o">=</span> <span class="n">mdl_f</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">response_f</span> <span class="o">=</span> <span class="n">mdl_f</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">response_f</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;on channel response&quot;</span><span class="p">,</span> <span class="s2">&quot;off channel response&quot;</span><span class="p">],</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/mnt/home/wbroderick/plenoptic/src/plenoptic/simulate/models/frontend.py:511: UserWarning: pretrained is True but cache_filt is False. Set cache_filt to True for efficiency unless you are fine-tuning.
  warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_6_1.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_6_1.png" />
</div>
</div>
</section>
<section id="Synthesizing-eigendistortions">
<h2>Synthesizing eigendistortions<a class="headerlink" href="#Synthesizing-eigendistortions" title="Link to this heading"></a></h2>
<section id="Front-end-model:-eigendistortion-synthesis">
<h3>Front-end model: eigendistortion synthesis<a class="headerlink" href="#Front-end-model:-eigendistortion-synthesis" title="Link to this heading"></a></h3>
<p>Now that we have our Front End model set up, we can synthesize eigendistortions! This is done easily just by calling <code class="docutils literal notranslate"><span class="pre">.synthesis()</span></code> after instantiating the <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> object. We’ll synthesize the top and bottom <code class="docutils literal notranslate"><span class="pre">k</span></code>, representing the most- and least-noticeable eigendistortions for this model.</p>
<p>The paper synthesizes the top and bottom <code class="docutils literal notranslate"><span class="pre">k=1</span></code> eigendistortions, but we’ll set <code class="docutils literal notranslate"><span class="pre">k&gt;1</span></code> so the algorithm converges/stabilizes faster.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># synthesize the top and bottom k distortions</span>
<span class="n">eigendist_f</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image_tensor</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">mdl_f</span><span class="p">)</span>
<span class="n">eigendist_f</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter_frontend</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/mnt/home/wbroderick/plenoptic/src/plenoptic/tools/validate.py:202: UserWarning: model is in training mode, you probably want to call eval() to switch to evaluation mode
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9ee750d11d104f4ab1ef70dd7a4ee0ae", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=3 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9ebf5755f5aa43e6890ffc2406009a43", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Bottom k=3 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
</section>
<section id="Front-end-model:-eigendistortion-display">
<h3>Front-end model: eigendistortion display<a class="headerlink" href="#Front-end-model:-eigendistortion-display" title="Link to this heading"></a></h3>
<p>Once synthesized, we can plot the distortion on the image using <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code>’s built-in display method. Feel free to adjust the constants <code class="docutils literal notranslate"><span class="pre">alpha_max</span></code> and <code class="docutils literal notranslate"><span class="pre">alpha_min</span></code> that scale the amount of each distortion on the image.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">eigendist_f</span><span class="o">.</span><span class="n">eigendistortions</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">vrange</span><span class="o">=</span><span class="s2">&quot;auto1&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;most-noticeable distortion&quot;</span><span class="p">,</span> <span class="s2">&quot;least-noticeable&quot;</span><span class="p">],</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">alpha_max</span><span class="p">,</span> <span class="n">alpha_min</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span>
<span class="n">f_max</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">eigendist_f</span><span class="p">,</span>
    <span class="n">eigenindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_max</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;img + </span><span class="si">{</span><span class="n">alpha_max</span><span class="si">}</span><span class="s2"> * max_dist&quot;</span><span class="p">,</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">f_min</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">eigendist_f</span><span class="p">,</span>
    <span class="n">eigenindex</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_min</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;img + </span><span class="si">{</span><span class="n">alpha_min</span><span class="si">}</span><span class="s2"> * min_dist&quot;</span><span class="p">,</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_10_0.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_10_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_10_1.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_10_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_10_2.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_10_2.png" />
</div>
</div>
</section>
<section id="VGG16:-eigendistortion-synthesis">
<h3>VGG16: eigendistortion synthesis<a class="headerlink" href="#VGG16:-eigendistortion-synthesis" title="Link to this heading"></a></h3>
<p>Following the lead of Berardino et al. (2017), let’s compare the Front End model’s eigendistortion to those of an early layer of VGG16! VGG16 takes as input color images, so we’ll need to repeat the grayscale parrot along the RGB color dimension.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a class that takes the nth layer output of a given model</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TorchVision</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_node</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">feature_extraction</span><span class="o">.</span><span class="n">create_feature_extractor</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">return_nodes</span><span class="o">=</span><span class="p">[</span><span class="n">return_node</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_node</span> <span class="o">=</span> <span class="n">return_node</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">return_node</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>VGG16 was trained on pre-processed ImageNet images with approximately zero mean and unit stdev, so we can preprocess our Parrot image the same way.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># VGG16</span>
<span class="k">def</span><span class="w"> </span><span class="nf">normalize</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;standardize the image for vgg16&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">img_tensor</span> <span class="o">-</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">img_tensor</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>


<span class="c1"># store these for later so we can un-normalize the image for display purposes</span>
<span class="n">orig_mean</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">orig_std</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">image_tensor</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">image_tensor3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">image_tensor</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># &quot;layer 3&quot; according to Berardino et al (2017)</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">VGG16_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">mdl_v</span> <span class="o">=</span> <span class="n">TorchVision</span><span class="p">(</span><span class="n">vgg</span><span class="p">,</span> <span class="s2">&quot;features.11&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">remove_grad</span><span class="p">(</span><span class="n">mdl_v</span><span class="p">)</span>
<span class="n">mdl_v</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">eigendist_v</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image_tensor3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">mdl_v</span><span class="p">)</span>
<span class="n">eigendist_v</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter_vgg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "11a21ad28ce3492badfa31b4b18781a9", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8713fdca1ab74ac4a9a9a08f3e8a942f", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="VGG16:-eigendistortion-display">
<h3>VGG16: eigendistortion display<a class="headerlink" href="#VGG16:-eigendistortion-display" title="Link to this heading"></a></h3>
<p>We can now display the most- and least-noticeable eigendistortions as before, then compare their quality to those of the Front-end model.</p>
<p>Since the distortions here were synthesized using a pre-processed (normalized) imagea, we can easily pass a function to unprocess the image. Since the previous eigendistortions were grayscale, we’ll just take the mean across RGB channels for VGG16-synthesized eigendistortions and display them as grayscale too.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">eigendist_v</span><span class="o">.</span><span class="n">eigendistortions</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">vrange</span><span class="o">=</span><span class="s2">&quot;auto1&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;most-noticeable distortion&quot;</span><span class="p">,</span> <span class="s2">&quot;least-noticeable&quot;</span><span class="p">],</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># create an image processing function to unnormalize the image and avg the channels to</span>
<span class="c1"># grayscale</span>
<span class="k">def</span><span class="w"> </span><span class="nf">unnormalize</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">orig_std</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="n">orig_mean</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">alpha_max</span><span class="p">,</span> <span class="n">alpha_min</span> <span class="o">=</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">100.0</span>

<span class="n">v_max</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">eigendist_v</span><span class="p">,</span>
    <span class="n">eigenindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_max</span><span class="p">,</span>
    <span class="n">process_image</span><span class="o">=</span><span class="n">unnormalize</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;img + </span><span class="si">{</span><span class="n">alpha_max</span><span class="si">}</span><span class="s2"> * most_noticeable_dist&quot;</span><span class="p">,</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">v_min</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">eigendist_v</span><span class="p">,</span>
    <span class="n">eigenindex</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_min</span><span class="p">,</span>
    <span class="n">process_image</span><span class="o">=</span><span class="n">unnormalize</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;img + </span><span class="si">{</span><span class="n">alpha_min</span><span class="si">}</span><span class="s2"> * least_noticeable_dist&quot;</span><span class="p">,</span>
    <span class="n">zoom</span><span class="o">=</span><span class="n">zoom</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_16_0.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_16_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_16_1.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_16_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_applications_Demo_Eigendistortion_16_2.png" src="../../_images/tutorials_applications_Demo_Eigendistortion_16_2.png" />
</div>
</div>
</section>
</section>
<section id="Final-thoughts">
<h2>Final thoughts<a class="headerlink" href="#Final-thoughts" title="Link to this heading"></a></h2>
<p>To rigorously test which of these model’s representations are more human-like, we’ll have to conduct a perceptual experiment. For now, we’ll just leave it to you to eyeball and decide which distortions are more or less noticeable!</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="09_Original_MAD.html" class="btn btn-neutral float-left" title="Reproducing Wang and Simoncelli, 2008 (MAD Competition)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../synthesis.html" class="btn btn-neutral float-right" title="Synthesis object design" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>