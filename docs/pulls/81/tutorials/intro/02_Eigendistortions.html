

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Eigendistortions &mdash; plenoptic 1.2.1.dev138 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f712bd2b"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=35a8b989"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Metamers" href="06_Metamer.html" />
    <link rel="prev" title="Citation Guide and Bibliography" href="../../citation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citation.html">Citation Guide and Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Eigendistortions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/intro/02_Eigendistortions.nblink.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Eigendistortions">
<h1>Eigendistortions<a class="headerlink" href="#Eigendistortions" title="Link to this heading"></a></h1>
<p><strong>Run notebook online with Binder:</strong><a class="reference external" href="https://mybinder.org/v2/gh/plenoptic-org/plenoptic/1.1.0?filepath=examples/02_Eigendistortions.ipynb"><img alt="Binder" src="http://mybinder.org/badge_logo.svg" /></a></p>
<p><strong>In this tutorial we will cover:</strong></p>
<ul class="simple">
<li><p>theory behind eigendistortions</p></li>
<li><p>how to use the <code class="docutils literal notranslate"><span class="pre">plenoptic.synthesize.eigendistortion.Eigendistortion</span></code> object</p></li>
<li><p>computing eigendistortions using a simple input and linear model</p></li>
<li><p>computing extremal eigendistortions for different layers of ResNet18</p></li>
</ul>
<section id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Link to this heading"></a></h2>
<p>How can we assess whether a model sees like we do? One way is to test whether they “notice” image distortions the same way as us. For a model, a noticeable distortion would be an image perturbation that elicits a change in its response. If our goal is to create models with human-like vision, then an image distortion that is (not) noticeable to a human should also (not) be noticeable to our models. Eigendistortions provide a framework with which to compare models to human visual perception of
distortions.</p>
<p><em>Berardino, A., Laparra, V., Ballé, J. and Simoncelli, E., 2017. Eigen-distortions of hierarchical representations. In Advances in neural information processing systems (pp. 3530-3539).</em></p>
<p><a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf">https://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf</a></p>
<p><a class="reference external" href="https://www.cns.nyu.edu/~lcv/eigendistortions/">https://www.cns.nyu.edu/~lcv/eigendistortions/</a></p>
<p><strong>See the last section of this notebook for more mathematical detail</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">plenoptic.synthesize.eigendistortion</span><span class="w"> </span><span class="kn">import</span> <span class="n">Eigendistortion</span>

<span class="c1"># so that relative sizes of axes created by po.imshow and others look right</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">72</span>


<span class="c1"># this notebook uses torchvision, which is an optional dependency.</span>
<span class="c1"># if this fails, install torchvision in your plenoptic environment</span>
<span class="c1"># and restart the notebook kernel.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">feature_extraction</span>
<span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span>
        <span class="s2">&quot;optional dependency torchvision not found!&quot;</span>
        <span class="s2">&quot; please install it in your plenoptic environment &quot;</span>
        <span class="s2">&quot;and restart the notebook kernel&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.
  return self._float_to_str(self.smallest_subnormal)
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/mnt/sw/nix/store/29h1dijh98y9ar6n8hxv78v8zz2pqfzf-python-3.11.7-view/lib/python3.11/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.
  return self._float_to_str(self.smallest_subnormal)
</pre></div></div>
</div>
</section>
<section id="Example-1:-Linear-model,-small-1D-input-%22image%22">
<h2>Example 1: Linear model, small 1D input “image”<a class="headerlink" href="#Example-1:-Linear-model,-small-1D-input-%22image%22" title="Link to this heading"></a></h2>
<section id="1.1)-Creating-the-model">
<h3>1.1) Creating the model<a class="headerlink" href="#1.1)-Creating-the-model" title="Link to this heading"></a></h3>
<p>The fundamental goal of computing eigendistortions is to understand how small changes (distortions) in inputs affect model outputs. Any model can be thought of as a black box mapping an input to an output, <span class="math notranslate nohighlight">\(f(x): x \in \mathbb{R}^n \mapsto y \in \mathbb{R}^m\)</span>, i.e. a function takes as input an n-dimensional vector <span class="math notranslate nohighlight">\(x\)</span> and outputs an m-dimensional vector <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The simplest model that achieves this is linear,</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{align}</dt><dd><p>y &amp;= f(x) = Mx, &amp;&amp; text{$Min mathbb{R^{mtimes n}}$}.</p>
</dd>
</dl>
<p>end{align}`</p>
<p>In this linear case, the Jacobian is fixed <span class="math notranslate nohighlight">\(J= \frac{\partial f}{\partial x}=M\)</span> for all possible inputs <span class="math notranslate nohighlight">\(x\)</span>. Can we <em>synthesize</em> a distortion <span class="math notranslate nohighlight">\(\epsilon\)</span> such that <span class="math notranslate nohighlight">\(f(x+\epsilon)\)</span> is maximally/minimally perturbed from the original <span class="math notranslate nohighlight">\(f(x)\)</span>? Yes! This would amount to finding the first and last eigenvectors of the Fisher information matrix, i.e. <span class="math notranslate nohighlight">\(J^TJ v = \lambda v\)</span>.</p>
<p>We’ll be working with the <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> object and its instance method, <code class="docutils literal notranslate"><span class="pre">synthesize()</span></code>.</p>
<p>Let’s make a linear PyTorch model and compute eigendistortions for a given input.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LinearModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The simplest model we can make.</span>
<span class="sd">    Its Jacobian should be the weight matrix of M, and the eigenvectors of the</span>
<span class="sd">    Fisher matrix are therefore the eigenvectors of M.T @ M&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># this computes y = x @ M.T</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="c1"># input vector dim (can you predict what the eigenvec/vals would be when n&lt;m or n=m?</span>
<span class="c1"># Feel free to try!)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># output vector dim</span>

<span class="n">mdl_linear</span> <span class="o">=</span> <span class="n">LinearModel</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">remove_grad</span><span class="p">(</span><span class="n">mdl_linear</span><span class="p">)</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">mdl_linear</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2">D Input&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">y0</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s2">&quot;C1o&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">m</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2">D Output&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_3_0.png" src="../../_images/tutorials_intro_02_Eigendistortions_3_0.png" />
</div>
</div>
</section>
<section id="1.2---Synthesizing-eigendistortions-of-linear-model">
<h3>1.2 - Synthesizing eigendistortions of linear model<a class="headerlink" href="#1.2---Synthesizing-eigendistortions-of-linear-model" title="Link to this heading"></a></h3>
<p>To compute the eigendistortions of this model, we can instantiate an <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> object with an input tensor and any PyTorch model with valid <code class="docutils literal notranslate"><span class="pre">forward</span></code> and <code class="docutils literal notranslate"><span class="pre">backward</span></code> methods. After that, we simply call the instance method <code class="docutils literal notranslate"><span class="pre">synthesize()</span></code> and choose the appropriate synthesis method. Normally our input has thousands of entries, but our input in this case is small (only n=25 entries), so we can compute the full <span class="math notranslate nohighlight">\(m \times n\)</span> Jacobian, and all the eigenvectors of the
<span class="math notranslate nohighlight">\(n \times n\)</span> Fisher matrix, <span class="math notranslate nohighlight">\(F=J^TJ\)</span>. The <code class="docutils literal notranslate"><span class="pre">synthesize</span></code> method does this for us and stores the outputs (eigendistortions, eigenvalues, eigenindex) of the synthesis.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">Eigendistortion</span><span class="o">.</span><span class="n">synthesize</span><span class="p">)</span>  <span class="c1"># fully documented</span>

<span class="n">eig_jac</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">mdl_linear</span>
<span class="p">)</span>  <span class="c1"># instantiate Eigendistortion object using an input and model</span>
<span class="n">eig_jac</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;exact&quot;</span><span class="p">)</span>  <span class="c1"># compute the entire Jacobian exactly</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Help on function synthesize in module plenoptic.synthesize.eigendistortion:

synthesize(self, method: Literal[&#39;exact&#39;, &#39;power&#39;, &#39;randomized_svd&#39;] = &#39;power&#39;, k: int = 1, max_iter: int = 1000, p: int = 5, q: int = 2, stop_criterion: float = 1e-07)
    Compute eigendistortions of Fisher Information Matrix with given input image.

    Parameters
    ----------
    method
        Eigensolver method. &#39;exact&#39; tries to do eigendecomposition directly (
        not recommended for very large inputs). &#39;power&#39; (default) uses the power
        method to compute first and last eigendistortions, with maximum number of
        iterations dictated by n_steps. &#39;randomized_svd&#39; uses randomized SVD to
        approximate the top k eigendistortions and their corresponding eigenvalues.
    k
        How many vectors to return using block power method or svd.
    max_iter
        Maximum number of steps to run for ``method=&#39;power&#39;`` in eigenvalue
        computation. Ignored for other methods.
    p
        Oversampling parameter for randomized SVD. k+p vectors will be sampled,
        and k will be returned. See docstring of ``_synthesize_randomized_svd``
        for more details including algorithm reference.
    q
        Matrix power parameter for randomized SVD. This is an effective trick for
        the algorithm to converge to the correct eigenvectors when the
        eigenspectrum does not decay quickly. See ``_synthesize_randomized_svd``
        for more details including algorithm reference.
    stop_criterion
        Used if ``method=&#39;power&#39;`` to check for convergence. If the L2-norm
        of the eigenvalues has changed by less than this value from one
        iteration to the next, we terminate synthesis.

</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/mnt/home/wbroderick/plenoptic/src/plenoptic/tools/validate.py:59: UserWarning: plenoptic&#39;s methods have mostly been tested on 4d inputs with shape torch.Size([n_batch, n_channels, im_height, im_width]). They should theoretically work with different dimensionality; if you have any problems, please open an issue at https://github.com/plenoptic-org/plenoptic/issues/new?template=bug_report.md
  warnings.warn(
/mnt/home/wbroderick/plenoptic/src/plenoptic/tools/validate.py:191: UserWarning: plenoptic&#39;s methods have mostly been tested on models which produce 3d or 4d outputs. They should theoretically work with different dimensionality; if you have any problems, please open an issue at https://github.com/plenoptic-org/plenoptic/issues/new?template=bug_report.md
  warnings.warn(
/mnt/home/wbroderick/plenoptic/src/plenoptic/tools/validate.py:202: UserWarning: model is in training mode, you probably want to call eval() to switch to evaluation mode
  warnings.warn(
</pre></div></div>
</div>
</section>
<section id="1.3---Comparing-our-synthesis-to-ground-truth">
<h3>1.3 - Comparing our synthesis to ground-truth<a class="headerlink" href="#1.3---Comparing-our-synthesis-to-ground-truth" title="Link to this heading"></a></h3>
<p>The Jacobian is in general a rectangular (not necessarily square) matrix <span class="math notranslate nohighlight">\(J\in \mathbb{R}^{m\times n}\)</span>. Since this is a linear model, let’s check if the computed Jacobian (stored as an attribute in the <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> object) matches the weight matrix <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>Since the eigendistortions are each 1D (vectors) in this example, we can display them all as an image where each column is an eigendistortion, each pixel is an entry of the eigendistortion, and the intensity is proportional to its value.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">jacobian</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">mdl_linear</span><span class="o">.</span><span class="n">M</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
    <span class="n">vmin</span><span class="o">=</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">jacobian</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
    <span class="n">vmax</span><span class="o">=</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">jacobian</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Solved Jacobian&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Linear model weight matrix&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Jacobian == weight matrix M?:&quot;</span><span class="p">,</span>
    <span class="n">eig_jac</span><span class="o">.</span><span class="n">jacobian</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mdl_linear</span><span class="o">.</span><span class="n">M</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Eigenvectors (aka eigendistortions) and associated eigenvectors are found in the</span>
<span class="c1"># distortions dict attribute</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">eigendistortions</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;coolwarm&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Eigendistortions&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Eigenvector index&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Entry&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Eigenvalues&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Eigenvector index&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Eigenvalue&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Jacobian == weight matrix M?: True
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_7_1.png" src="../../_images/tutorials_intro_02_Eigendistortions_7_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_7_2.png" src="../../_images/tutorials_intro_02_Eigendistortions_7_2.png" />
</div>
</div>
</section>
<section id="1.4---What-do-these-eigendistortions-mean?">
<h3>1.4 - What do these eigendistortions <em>mean</em>?<a class="headerlink" href="#1.4---What-do-these-eigendistortions-mean?" title="Link to this heading"></a></h3>
<p>The first eigenvector (with the largest eigenvalue) is the direction in which we can distort our input <span class="math notranslate nohighlight">\(x\)</span> and change the response of the model the <em>most</em>, i.e. its most noticeable distortion. For the last eigenvector, since its associated eigenvalue is 0, then <em>no change in response occurs</em> when we distort the input in that direction, i.e. <span class="math notranslate nohighlight">\(f(x+\epsilon)=f(x)\)</span>. So this distortion would be <em>imperceptible</em> to the model.</p>
<p>In most cases, our input would be much larger. An <span class="math notranslate nohighlight">\(n\times n\)</span> image has <span class="math notranslate nohighlight">\(n^2\)</span> entries, meaning the Fisher matrix is <span class="math notranslate nohighlight">\(n^2 \times n^2\)</span>, and therefore <span class="math notranslate nohighlight">\(n^2\)</span> possible eigendistortions – certainly too large to store in memory. We need to instead resort to numerical methods to compute the eigendistortions. To do this, we can just set our synthesis <code class="docutils literal notranslate"><span class="pre">method='power'</span></code> to estimate the first eigenvector (most noticeable distortion) and last eigenvector (least noticeable
distortion) for the image.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eig_pow</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">mdl_linear</span><span class="p">)</span>
<span class="n">eig_pow</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">eigdist_pow</span> <span class="o">=</span> <span class="n">eig_pow</span><span class="o">.</span><span class="n">eigendistortions</span>
<span class="n">eigdist_jac</span> <span class="o">=</span> <span class="n">eig_jac</span><span class="o">.</span><span class="n">eigendistortions</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Indices of computed eigenvectors: </span><span class="si">{</span><span class="n">eig_pow</span><span class="o">.</span><span class="n">eigenindex</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eig_pow</span><span class="o">.</span><span class="n">eigenindex</span><span class="p">,</span> <span class="n">eig_pow</span><span class="o">.</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Power&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eig_jac</span><span class="o">.</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="s2">&quot;.-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Jacobian&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Power method vs Jacobian&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Eigenvector index&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Eigenvalue&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Synth. method&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eigdist_pow</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">eigdist_jac</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Difference in first eigendists&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">eigdist_pow</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eigdist_jac</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Difference in last eigendists&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">eigdist_jac</span> <span class="o">@</span> <span class="n">eigdist_pow</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Power method&#39;s last eigenvec projected on all Jacobian method&#39;s eigenvec&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Eigenvector index&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Projection&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Are the first eigendistortions the same?&quot;</span><span class="p">,</span>
    <span class="n">eigdist_pow</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">eigdist_jac</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Are the last eigendistortions the same?&quot;</span><span class="p">,</span>
    <span class="n">eigdist_pow</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">eigdist_jac</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># find eigendistortions of Jacobian-method whose eigenvalues are zero</span>
<span class="n">ind_zero</span> <span class="o">=</span> <span class="n">eig_jac</span><span class="o">.</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b2af1c2135c04e6489b723e713523905", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=1 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "fd100d7751624dc9b556d0be526b93a0", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Bottom k=1 eigendists computed | Stop criterion 1.00E-07 reached.
Indices of computed eigenvectors: tensor([ 0, 24])

Are the first eigendistortions the same? True
Are the last eigendistortions the same? False
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_9_4.png" src="../../_images/tutorials_intro_02_Eigendistortions_9_4.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_9_5.png" src="../../_images/tutorials_intro_02_Eigendistortions_9_5.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_9_6.png" src="../../_images/tutorials_intro_02_Eigendistortions_9_6.png" />
</div>
</div>
<p>The power method’s first eigendistortion matches the ground-truth first eigendistortion obtained via the Jacobian solve. And while the last eigendistortions don’t match, the last power method eigendistortion lies in the span of all the eigendistortions whose eigenvalues are zero. Each of these eigendistortions whose eigenvalues are zero are equivalent. Any distortion of <span class="math notranslate nohighlight">\(x\)</span> in the span of these eigendistortions would result in <em>no change</em> in the model output, and would therefore be
imperceptible to the model.</p>
</section>
<section id="1.5---The-Fisher-information-matrix-is-a-locally-adaptive">
<h3>1.5 - The Fisher information matrix is a locally adaptive<a class="headerlink" href="#1.5---The-Fisher-information-matrix-is-a-locally-adaptive" title="Link to this heading"></a></h3>
<p>Different inputs should in general have different sets of eigendistortions – a noticible distortion in one image would not necessarily be noticeable in a different image. The only case where they should be the same regardless of input is when the model is fully linear, as in this simple example. So let’s check if the Jacobian at a different input still equals the weight matrix <span class="math notranslate nohighlight">\(M\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>  <span class="c1"># generate some random input</span>
<span class="n">eig_jac2</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">mdl_linear</span><span class="p">)</span>
<span class="n">eig_jac2</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;exact&quot;</span>
<span class="p">)</span>  <span class="c1"># since the model is linear, the Jacobian should be the exact same as before</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Does the jacobian at x1 still equal the model weight matrix?&quot;</span>
    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">eig_jac2</span><span class="o">.</span><span class="n">jacobian</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mdl_linear</span><span class="o">.</span><span class="n">M</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Does the jacobian at x1 still equal the model weight matrix? True
</pre></div></div>
</div>
</section>
</section>
<section id="Example-2:-Which-layer-of-ResNet-is-a-better-model-of-human-visual-distortion-perception?">
<h2>Example 2: Which layer of ResNet is a better model of human visual distortion perception?<a class="headerlink" href="#Example-2:-Which-layer-of-ResNet-is-a-better-model-of-human-visual-distortion-perception?" title="Link to this heading"></a></h2>
<p>Now that we understand what eigendistortions are and how the <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> class works, let’s compute them real images using a more complex model like Vgg16. The response vector <span class="math notranslate nohighlight">\(y\)</span> doesn’t necessarily have to be the output of the last layer of the model; we can also compute Eigendistortions for intermediate model layers too. Let’s synthesize distortions for an image using different layers of Vgg16 to see which layer produces extremal eigendistortions that align more with human
perception.</p>
<section id="2.1---Load-an-example-an-image">
<h3>2.1 - Load an example an image<a class="headerlink" href="#2.1---Load-an-example-an-image" title="Link to this heading"></a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># this will be the img_height and width of the input, you can change this to</span>
<span class="c1"># accommodate your machine</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">color_wheel</span><span class="p">()</span>
<span class="c1"># center crop the image to nxn</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">center_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_14_0.png" src="../../_images/tutorials_intro_02_Eigendistortions_14_0.png" />
</div>
</div>
</section>
<section id="2.2---Instantiate-models-and-Eigendistortion-objects">
<h3>2.2 - Instantiate models and Eigendistortion objects<a class="headerlink" href="#2.2---Instantiate-models-and-Eigendistortion-objects" title="Link to this heading"></a></h3>
<p>Let’s make a wrapper class that can return the nth layer output of vgg. We’re going to use this to compare eigendistortions synthesized using different layers of Vgg as models for distortion perception.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TorchVision</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_node</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span> <span class="o">=</span> <span class="n">feature_extraction</span><span class="o">.</span><span class="n">create_feature_extractor</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">return_nodes</span><span class="o">=</span><span class="p">[</span><span class="n">return_node</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_node</span> <span class="o">=</span> <span class="n">return_node</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">return_node</span><span class="p">]</span>


<span class="c1"># different potential models of human visual perception of distortions</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">resnet18_a</span> <span class="o">=</span> <span class="n">TorchVision</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="s2">&quot;maxpool&quot;</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">remove_grad</span><span class="p">(</span><span class="n">resnet18_a</span><span class="p">)</span>
<span class="n">resnet18_a</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">resnet18_b</span> <span class="o">=</span> <span class="n">TorchVision</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="s2">&quot;layer2&quot;</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">remove_grad</span><span class="p">(</span><span class="n">resnet18_b</span><span class="p">)</span>
<span class="n">resnet18_b</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">ed_resneta</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">resnet18_a</span><span class="p">)</span>
<span class="n">ed_resnetb</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">resnet18_b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="2.3---Synthesizing-distortions">
<h3>2.3 - Synthesizing distortions<a class="headerlink" href="#2.3---Synthesizing-distortions" title="Link to this heading"></a></h3>
<p>The input dimensionality in this example is huge compared to our linear model example – it is <span class="math notranslate nohighlight">\((\textrm{n_chan} \times \textrm{img_height} \times \textrm{img_width})^2\)</span>, meaning the Fisher matrix is too massive to compute exactly. We must turn to iterative methods. Let’s synthesize the extremal eigendistortions for this picture of Einstein using the different layers of ResNet as defined above.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bump up n_steps if you wish</span>
<span class="n">ed_resneta</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">ed_resnetb</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2bf1d5c6cfb84c44a493caffd47f9100", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=1 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0290d1bac4c84c6983b68c1096d32c59", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0d5000c2266a4be4a1e810a0bce6a93f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=1 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b2aabeb167414b83b5c8d6c0fe4a7403", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="2.4---Visualizing-eigendistortions">
<h3>2.4 - Visualizing eigendistortions<a class="headerlink" href="#2.4---Visualizing-eigendistortions" title="Link to this heading"></a></h3>
<p>Let’s display the eigendistortions. <code class="docutils literal notranslate"><span class="pre">Eigendistortion</span></code> has an instance method <code class="docutils literal notranslate"><span class="pre">display</span></code> that will display a 2x3 subplot figure of images. The top row shows the original image on the left, the synthesized maximal eigendistortion on the right, and some constsant <span class="math notranslate nohighlight">\(\alpha\)</span> times the eigendistortion added to the image in the middle panel. The bottow row has a similar layout, but displays the minimal eigendistortion. Let’s display the eigendistortions for both models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span><span class="n">ed_resneta</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span><span class="n">ed_resneta</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_20_0.png" src="../../_images/tutorials_intro_02_Eigendistortions_20_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_20_1.png" src="../../_images/tutorials_intro_02_Eigendistortions_20_1.png" />
</div>
</div>
</section>
<section id="2.5---Which-synthesized-extremal-eigendistortions-better-characterize-human-perception?">
<h3>2.5 - Which synthesized extremal eigendistortions better characterize human perception?<a class="headerlink" href="#2.5---Which-synthesized-extremal-eigendistortions-better-characterize-human-perception?" title="Link to this heading"></a></h3>
<p>Let’s compare eigendistortions within a model first. One thing we immediately notice is that the first eigendistortion (labeled <code class="docutils literal notranslate"><span class="pre">maxdist</span></code>) is indeed more noticeable than <code class="docutils literal notranslate"><span class="pre">mindist</span></code>. <code class="docutils literal notranslate"><span class="pre">maxdist</span></code> is localized to a single portion of the image, and has lower, more prominent spatial frequency content than <code class="docutils literal notranslate"><span class="pre">mindist</span></code>. <code class="docutils literal notranslate"><span class="pre">mindist</span></code> looks more like high frequency noise distributed across the image.</p>
<p>But how do the distortions compare between models – which model better characterizes human visual perception of distortions? The only way to truly this is to run an experiment and ask human observers which distortions are most/least noticeable to them. The best model should produce a maximally noticeable distortion that is more noticeable than other models’ maximally noticeable distortions, and its minimally noticeable distortion should be less noticeable than other models’ minimally noticeable
distortions.</p>
<p>See Berardino et al. 2017 for more details.</p>
</section>
<section id="2.6---Synthesizing-distortions-for-other-images">
<h3>2.6 - Synthesizing distortions for other images<a class="headerlink" href="#2.6---Synthesizing-distortions-for-other-images" title="Link to this heading"></a></h3>
<p>Remember the Fisher matrix is locally adaptive, meaning that a different image should have a different set of eigendistortions. Let’s finish off this notebook with another set of extremal eigendistortions for these two Vgg16 layers on a different image.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">curie</span><span class="p">()</span>

<span class="c1"># center crop the image to nxn</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">center_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1"># because this is a grayscale image but ResNet expects a color image,</span>
<span class="c1"># need to duplicate along the color dimension</span>
<span class="n">img3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ed_resneta</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">img3</span><span class="p">,</span> <span class="n">resnet18_a</span><span class="p">)</span>
<span class="n">ed_resnetb</span> <span class="o">=</span> <span class="n">Eigendistortion</span><span class="p">(</span><span class="n">img3</span><span class="p">,</span> <span class="n">resnet18_b</span><span class="p">)</span>

<span class="n">ed_resneta</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">ed_resnetb</span><span class="o">.</span><span class="n">synthesize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>

<span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Original&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3e5e4a90bdf54b5484862b2e5f33d080", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=1 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "769fab9b013b40a396a539c062c0bd17", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "fc85ba3bd96f4742935c73233f850200", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Top k=1 eigendists computed | Stop criterion 1.00E-07 reached.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1687e34d29a64255ad8e8faf08e64738", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_22_6.png" src="../../_images/tutorials_intro_02_Eigendistortions_22_6.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">ed_resneta</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;top eigendist&quot;</span>
<span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">ed_resneta</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;bottom eigendist&quot;</span>
<span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">ed_resnetb</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;top eigendist&quot;</span>
<span class="p">)</span>
<span class="n">po</span><span class="o">.</span><span class="n">synth</span><span class="o">.</span><span class="n">eigendistortion</span><span class="o">.</span><span class="n">display_eigendistortion</span><span class="p">(</span>
    <span class="n">ed_resnetb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;bottom eigendist&quot;</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_23_0.png" src="../../_images/tutorials_intro_02_Eigendistortions_23_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_23_1.png" src="../../_images/tutorials_intro_02_Eigendistortions_23_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_23_2.png" src="../../_images/tutorials_intro_02_Eigendistortions_23_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_intro_02_Eigendistortions_23_3.png" src="../../_images/tutorials_intro_02_Eigendistortions_23_3.png" />
</div>
</div>
</section>
</section>
<section id="Appendix:-More-mathematical-detail">
<h2>Appendix: More mathematical detail<a class="headerlink" href="#Appendix:-More-mathematical-detail" title="Link to this heading"></a></h2>
<p>If we have a model that takes an N-dimensional input and outputs an M-dimensional response, then its Jacobian, <span class="math notranslate nohighlight">\(J=\frac{\partial f}{\partial x}\)</span>, is an <span class="math notranslate nohighlight">\(M\times N\)</span> matrix of partial derivatives that tells us how much a change in each entry of the input would change each entry of the output. With the assumption of additive Gaussian noise in the output space Fisher Information Matrix, <span class="math notranslate nohighlight">\(F\)</span>, is a symmetric positive semi-definite, <span class="math notranslate nohighlight">\(N\times N\)</span> matrix computed using the
Jacobian, <span class="math notranslate nohighlight">\(F=J^TJ\)</span>. If you are familiar with linear algebra, you might notice that the eigenvectors of <span class="math notranslate nohighlight">\(F\)</span> are the right singular vectors of the Jacobian. Thus, an eigendecomposition <span class="math notranslate nohighlight">\(F=V\Lambda V\)</span> yields directions of the <em>input space</em> (vectors in <span class="math notranslate nohighlight">\(V\)</span>) along which changes in the <em>output space</em> are rank-ordered by entries in diagonal matrix <span class="math notranslate nohighlight">\(\Lambda\)</span>.</p>
<p>Given some input image <span class="math notranslate nohighlight">\(x_0\)</span>, an <strong>eigendistortion</strong> is an additive perturbation, <span class="math notranslate nohighlight">\(\epsilon\)</span>, in the <em>input domain</em> that changes the response in a model’s <em>output domain</em> of interest (e.g. an intermediate layer of a neural net, the output of a nonlinear model, etc.). These perturbations are named <em>eigendistortions</em> because they push <span class="math notranslate nohighlight">\(x_0\)</span> along eigenvectors of the Fisher Information Matrix. So we expect distortions <span class="math notranslate nohighlight">\(x_0\)</span> along the direction of the eigenvector with the
maximum eigenvalue will change the representation the <em>most</em>, and distortions along the eigenvector with the minimum eigenvalue will change the representation the <em>least</em>. (And pushing along intermediate eigenvectors will change the representation by an intermediate amount.)</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../citation.html" class="btn btn-neutral float-left" title="Citation Guide and Bibliography" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="06_Metamer.html" class="btn btn-neutral float-right" title="Metamers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>