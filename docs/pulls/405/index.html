

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>plenoptic &mdash; plenoptic 1.3.2.dev217 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=a2d047e6" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=4ba52da5"></script>
      <script src="_static/doctools.js?v=fd6eb6e6"></script>
      <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=6dbb43f8"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide and Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/MAD_Competition_1.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/MAD_Competition_2.html">MAD Competition Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/Metamer.html">Metamers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_index.html">Portilla-Simoncelli Texture Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility and Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">For Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">plenoptic</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="plenoptic">
<span id="index-doc"></span><h1>plenoptic<a class="headerlink" href="#plenoptic" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://pypi.org/project/plenoptic/"><img alt="PyPI Version" src="https://img.shields.io/pypi/v/plenoptic.svg" /></a>
<a class="reference external" href="https://anaconda.org/conda-forge/plenoptic"><img alt="Anaconda-Server Badge" src="https://anaconda.org/conda-forge/plenoptic/badges/version.svg" /></a>
<a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/LICENSE"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-yellow.svg" /></a>
<img alt="Python version" src="https://img.shields.io/badge/python-3.10%7C3.11%7C3.12-blue.svg" />
<a class="reference external" href="https://github.com/plenoptic-org/plenoptic/actions?query=workflow%3Abuild"><img alt="Build Status" src="https://github.com/plenoptic-org/plenoptic/workflows/build/badge.svg" /></a>
<a class="reference external" href="https://doi.org/10.5281/zenodo.10151130"><img alt="DOI" src="https://zenodo.org/badge/DOI/10.5281/zenodo.10151130.svg" /></a>
<a class="reference external" href="https://codecov.io/gh/plenoptic-org/plenoptic"><img alt="codecov" src="https://codecov.io/gh/plenoptic-org/plenoptic/branch/main/graph/badge.svg?token=EDtl5kqXKA" /></a>
<a class="reference external" href="https://mybinder.org/v2/gh/plenoptic-org/plenoptic/1.1.0?filepath=examples"><img alt="Binder" src="https://mybinder.org/badge_logo.svg" /></a>
<a class="reference external" href="https://www.repostatus.org/#active"><img alt="Project Status: Active – The project has reached a stable, usable state and is being actively developed." src="https://www.repostatus.org/badges/latest/active.svg" /></a>
<a class="reference external" href="https://github.com/astral-sh/ruff"><img alt="Code style: Ruff" src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/format.json" /></a></p>
<figure class="align-center" style="width: 100%">
<img alt="plenoptic logo" src="_images/Plenoptic_Logo_CMYK_Full_Wide.svg" />
</figure>
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is a python library for model-based synthesis of perceptual stimuli. For <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code>, models are those of visual <a class="footnote-reference brackets" href="#footnote-1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> information processing: they accept an image<a class="footnote-reference brackets" href="#footnote-2" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> as input, perform some computations, and return some output, which can be mapped to neuronal firing rate, fMRI BOLD response, behavior on some task, image category, etc. The intended audience is researchers in neuroscience, psychology, and machine learning. The generated stimuli enable interpretation of model properties through examination of features that are enhanced, suppressed, or discarded. More importantly, they can facilitate the scientific process, through use in further perceptual or neural experiments aimed at validating or falsifying model predictions.</p>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>If you are unfamiliar with stimulus synthesis, see the <a class="reference internal" href="conceptual_intro.html#conceptual-intro"><span class="std std-ref">Conceptual Introduction</span></a> for an in-depth introduction.</p></li>
<li><p>Otherwise, see the <a class="reference internal" href="tutorials/quickstart.html#quickstart-nb"><span class="std std-ref">Quickstart</span></a> tutorial.</p></li>
</ul>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h3>
<p>The best way to install <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is via <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>plenoptic
</pre></div>
</div>
<p>or <code class="docutils literal notranslate"><span class="pre">conda</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>plenoptic<span class="w"> </span>-c<span class="w"> </span>conda-forge
</pre></div>
</div>
<p>See the <a class="reference internal" href="install.html#install-doc"><span class="std std-ref">Installation</span></a> page for more details, including how to set up an isolated virtual environment (recommended).</p>
</section>
</section>
<section id="contents">
<span id="package-contents"></span><h2>Contents<a class="headerlink" href="#contents" title="Link to this heading"></a></h2>
<figure class="align-default" style="width: 100%">
<img alt="The three synthesis methods included in plenoptic" src="_images/example_synth.svg" />
</figure>
<section id="synthesis-methods">
<h3>Synthesis methods<a class="headerlink" href="#synthesis-methods" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/intro/Metamer.html#metamer-nb"><span class="std std-ref">Metamers</span></a>: given a model and a reference image, stochastically generate a new image whose model representation is identical to that of the reference image (a “metamer”, as originally defined in the literature on Trichromacy). This method makes explicit those features that the model retains/discards.</p>
<ul>
<li><p>Example papers: <span id="id3"><a class="reference internal" href="citation.html#id27" title="Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International journal of computer vision, 40(1):49–70, 2000.">Portilla and Simoncelli, 2000</a></span>, <span id="id4"><a class="reference internal" href="citation.html#id21" title="Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature Neuroscience, 14(9):1195–1201, aug 2011. arXiv:PMC3164938, doi:10.1038/nn.2889.">Freeman and Simoncelli, 2011</a></span>, <span id="id5"><a class="reference internal" href="citation.html#id18" title="Arturo Deza, Aditya Jonnalagadda, and Miguel P. Eckstein. Towards metamerism via foveated style transfer. In International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=BJzbG20cFQ.">Deza <em>et al.</em>, 2019</a></span>, <span id="id6"><a class="reference internal" href="citation.html#id20" title="Jenelle Feather, Alex Durango, Ray Gonzalez, and Josh McDermott. Metamers of neural networks reveal divergence from human perceptual systems. In NeurIPS, 10078–10089. 2019.">Feather <em>et al.</em>, 2019</a></span>, <span id="id7"><a class="reference internal" href="citation.html#id32" title="Thomas SA Wallis, Christina M Funke, Alexander S Ecker, Leon A Gatys, Felix A Wichmann, and Matthias Bethge. Image content is more important than bouma's law for scene metamers. eLife, apr 2019. URL: https://doi.org/10.7554/elife.42512, doi:10.7554/elife.42512.">Wallis <em>et al.</em>, 2019</a></span>, <span id="id8"><a class="reference internal" href="citation.html#id36" title="Corey M. Ziemba and Eero P. Simoncelli. Opposing effects of selectivity and invariance in peripheral vision. Nature Communications, jul 2021. URL: https://doi.org/10.1038/s41467-021-24880-5, doi:10.1038/s41467-021-24880-5.">Ziemba and Simoncelli, 2021</a></span></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="tutorials/intro/Eigendistortions.html#eigendistortion-nb"><span class="std std-ref">Eigendistortions</span></a>: given a model and a reference image, compute the image perturbations that produce the smallest/largest change in the model response space. These are the image changes to which the model is least/most sensitive, respectively.</p>
<ul>
<li><p>Example papers: <span id="id9"><a class="reference internal" href="citation.html#id16" title="A Berardino, V Laparra, J Ballé, and E P Simoncelli. Eigen-distortions of hierarchical representations. In I Guyon, UV Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors, Adv. Neural Information Processing Systems (NIPS*17), volume 30, 1–10. Curran Associates, Inc., Dec 2017. Presented at: Neural Information Processing Systems 30, Dec 2017, Long Beach, CA.">Berardino <em>et al.</em>, 2017</a></span></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="tutorials/intro/MAD_Competition_2.html#mad-nb"><span class="std std-ref">Maximal differentiation (MAD) competition</span></a>: given a reference image and two models that measure distance
between images, generate pairs of images that optimally differentiate the models. Specifically, synthesize a pair of images that are equi-distant from the reference image according to model-1, but maximally/minimally distant according to model-2. Synthesize a second pair with the roles of the two models reversed. This method allows for efficient comparison of two metrics, highlighting the aspects in which their sensitivities most differ.</p>
<ul>
<li><p>Example papers: <span id="id10"><a class="reference internal" href="citation.html#id35" title="Z Wang and E P Simoncelli. Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual discriminability. Journal of Vision, 8(12):1–13, Sep 2008. arXiv:PMC4143340, doi:10.1167/8.12.8.">Wang and Simoncelli, 2008</a></span></p></li>
</ul>
</li>
</ul>
</section>
<section id="models-metrics-and-model-components">
<h3>Models, Metrics, and Model Components<a class="headerlink" href="#models-metrics-and-model-components" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Steerable pyramid, <span id="id11"><a class="reference internal" href="citation.html#id30" title="E P Simoncelli and W T Freeman. The steerable pyramid: A flexible architecture for multi-scale derivative computation. In Proc 2nd IEEE Int'l Conf on Image Proc (ICIP), volume III, 444–447. Washington, DC, Oct 23-26 1995. IEEE Sig Proc Society. doi:10.1109/ICIP.1995.537667.">Simoncelli and Freeman, 1995</a>, <a class="reference internal" href="citation.html#id29" title="E P Simoncelli, W T Freeman, E H Adelson, and D J Heeger. Shiftable multi-scale transforms. IEEE Trans. Information Theory, 38(2):587–607, Mar 1992. Special Issue on Wavelets. doi:10.1109/18.119725.">Simoncelli <em>et al.</em>, 1992</a></span>, is a multi-scale oriented image decomposition. Images are decomposed with a family of oriented filters, localized in space and frequency, similar to the “Gabor functions” commonly used to model receptive fields in primary visual cortex. The critical difference is that the pyramid organizes these filters so as to efficiently cover the 4D space of (x,y) positions, orientations, and scales, enabling efficient interpolation and interpretation (<a class="reference external" href="https://www.cns.nyu.edu/~eero/STEERPYR/">further info</a>). See the <a class="reference external" href="https://pyrtools.readthedocs.io/en/latest/index.html">pyrtools documentation</a> for more details on python tools for image pyramids in general and the steerable pyramid in particular.</p></li>
<li><p>Portilla-Simoncelli texture model, <span id="id12"><a class="reference internal" href="citation.html#id27" title="Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International journal of computer vision, 40(1):49–70, 2000.">Portilla and Simoncelli, 2000</a></span>, which computes a set of image statistics that capture the appearance of visual textures (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/texture/">further info</a>).</p></li>
<li><p>Structural Similarity Index (SSIM), <span id="id13"><a class="reference internal" href="citation.html#id34" title="Z. Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, apr 2004. URL: https://doi.org/10.1109 \%2Ftip.2003.819861, doi:10.1109/tip.2003.819861.">Wang <em>et al.</em>, 2004</a></span>, is a perceptual similarity metric, that takes two images and returns a value between -1 (totally different) and 1 (identical) reflecting their similarity (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim">further info</a>).</p></li>
<li><p>Multiscale Structural Similarity Index (MS-SSIM), <span id="id14"><a class="reference internal" href="citation.html#id33" title="Z. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers, 2003. IEEE, 2003. URL: https://doi.org/10.1109/acssc.2003.1292216, doi:10.1109/acssc.2003.1292216.">Wang <em>et al.</em>, 2003</a></span>, is an extension of SSIM that operates jointly over multiple scales.</p></li>
<li><p>Normalized Laplacian distance, <span id="id15"><a class="reference internal" href="citation.html#id24" title="Valero Laparra, Johannes Ballé, Alexander Berardino, and Eero P Simoncelli. Perceptual image quality assessment using a normalized laplacian pyramid. Electronic Imaging, 28(16):1-6, feb 2016. URL: http://dx.doi.org/10.2352/issn.2470-1173.2016.16.hvei-103, doi:10.2352/issn.2470-1173.2016.16.hvei-103.">Laparra <em>et al.</em>, 2016</a>, <a class="reference internal" href="citation.html#id25" title="Valero Laparra, Alexander Berardino, Johannes Ballé, and Eero P. Simoncelli. Perceptually optimized image rendering. Journal of the Optical Society of America A, 34(9):1511, aug 2017. URL: https://doi.org/10.1364\ \%2Fjosaa.34.001511, doi:10.1364/josaa.34.001511.">Laparra <em>et al.</em>, 2017</a></span>, is a perceptual distance metric based on transformations associated with the early visual system: local luminance subtraction and local contrast gain control, at six scales (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/NLPyr/">further info</a>).</p></li>
</ul>
</section>
</section>
<section id="getting-help">
<h2>Getting help<a class="headerlink" href="#getting-help" title="Link to this heading"></a></h2>
<p>We communicate via several channels on Github:</p>
<ul class="simple">
<li><p>To report a bug, open an <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/issues">issue</a>.</p></li>
<li><p>To send suggestions for extensions or enhancements, please post in the <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions/categories/ideas">ideas section</a> of discussions first. We’ll discuss it there and, if we decide to pursue it, open an issue to track progress.</p></li>
<li><p>To ask usage questions, discuss broad issues, or show off what you’ve made with plenoptic, go to <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions">Discussions</a>.</p></li>
<li><p>To contribute to the project, see the <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/CONTRIBUTING.md">contributing guide</a>.</p></li>
</ul>
<p>In all cases, we request that you respect our <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/CODE_OF_CONDUCT.md">code of conduct</a>.</p>
</section>
<section id="citing-us">
<h2>Citing us<a class="headerlink" href="#citing-us" title="Link to this heading"></a></h2>
<p>If you use <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> in a published academic article or presentation, please cite us! See the <a class="reference internal" href="citation.html#citation-doc"><span class="std std-ref">Citation Guide and Bibliography</span></a> for more details.</p>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide and Bibliography</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/MAD_Competition_1.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/MAD_Competition_2.html">MAD Competition Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/Metamer.html">Metamers</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_index.html">Portilla-Simoncelli Texture Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_intro.html">What is a visual texture?</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_basic_synthesis.html">Using the Portilla-Simoncelli model in plenoptic</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_examples.html">Example Portilla-Simoncelli metamers from different texture classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_understand_stats.html">Understanding Portilla-Simoncelli model statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_optimization.html">Portilla-Simoncelli optimization details</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_limitations.html">Portilla-Simoncelli Model Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_matlab_differences.html">Notable differences between Matlab and Plenoptic Implementations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials/models/portilla_simoncelli/ps_extensions.html">More than just texture metamers</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility and Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">For Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
</ul>
</div>
<p>This package is supported by the <a class="reference external" href="https://www.simonsfoundation.org/flatiron/center-for-computational-neuroscience/">Center for Computational Neuroscience</a>, in the Flatiron Institute of the Simons Foundation.</p>
<img alt="Flatiron Institute Center for Computational Neuroscience logo" class="align-center" src="_images/CCN-logo-wText.png" />
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnote-1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>These methods also work with auditory models, such as in <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html">Feather et al., 2019</a> though we haven’t yet implemented examples. If you’re interested, please post in <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions)">Discussions</a>!</p>
</aside>
<aside class="footnote brackets" id="footnote-2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Here and throughout the documentation, we use “image” to describe the input. The models and metrics that are included in plenoptic are intended to work on images, represented as 4d tensors. However, the synthesis methods should also work on videos (5d tensors), audio (3d tensors) and more! If you have a problem using a tensor with different dimensionality, please <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/issues/new?template=bug_report.md">open an issue</a>!</p>
</aside>
</aside>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2025, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>