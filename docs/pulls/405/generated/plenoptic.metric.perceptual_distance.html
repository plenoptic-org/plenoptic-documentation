

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>plenoptic.metric.perceptual_distance &mdash; plenoptic 1.3.2.dev217 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a2d047e6" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=4ba52da5"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=6dbb43f8"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citation.html">Citation Guide and Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro/Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro/MAD_Competition_1.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro/MAD_Competition_2.html">MAD Competition Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro/Metamer.html">Metamers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/portilla_simoncelli/ps_index.html">Portilla-Simoncelli Texture Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/applications/Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reproducibility.html">Reproducibility and Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">For Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../synthesis.html">Synthesis object design</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">plenoptic.metric.perceptual_distance</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/plenoptic.metric.perceptual_distance.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-plenoptic.metric.perceptual_distance">
<span id="plenoptic-metric-perceptual-distance"></span><h1>plenoptic.metric.perceptual_distance<a class="headerlink" href="#module-plenoptic.metric.perceptual_distance" title="Link to this heading"></a></h1>
<p>Metrics designed to model human perceptual distance.</p>
<p>Metrics that model human perceptual distance seek to answer the question “how different
do humans find these two images?”.</p>
<p class="rubric">Functions</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#plenoptic.metric.perceptual_distance.ms_ssim" title="plenoptic.metric.perceptual_distance.ms_ssim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ms_ssim</span></code></a>(img1, img2[, power_factors])</p></td>
<td><p>Multiscale structural similarity index (MS-SSIM).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#plenoptic.metric.perceptual_distance.nlpd" title="plenoptic.metric.perceptual_distance.nlpd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nlpd</span></code></a>(img1, img2)</p></td>
<td><p>Compute the normalized Laplacian Pyramid Distance.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#plenoptic.metric.perceptual_distance.normalized_laplacian_pyramid" title="plenoptic.metric.perceptual_distance.normalized_laplacian_pyramid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normalized_laplacian_pyramid</span></code></a>(img)</p></td>
<td><p>Compute the normalized Laplacian Pyramid using pre-optimized parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim" title="plenoptic.metric.perceptual_distance.ssim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ssim</span></code></a>(img1, img2[, weighted, pad])</p></td>
<td><p>Compute the structural similarity index.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim_map" title="plenoptic.metric.perceptual_distance.ssim_map"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ssim_map</span></code></a>(img1, img2)</p></td>
<td><p>Structural similarity index map.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="plenoptic.metric.perceptual_distance.ssim">
<span class="sig-name descname"><span class="pre">ssim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weighted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/plenoptic/metric/perceptual_distance.html#ssim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plenoptic.metric.perceptual_distance.ssim" title="Link to this definition"></a></dt>
<dd><p>Compute the structural similarity index.</p>
<p>As described in Wang et al., 2004 <a class="footnote-reference brackets" href="#id8" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, the structural similarity index (SSIM) is a
perceptual distance metric, giving the distance between two images. SSIM is based on
three comparison measurements between the two images: luminance, contrast, and
structure. All of these are computed convolutionally across the images. See the
references for more information.</p>
<p>This implementation follows the original implementation, as found online <a class="footnote-reference brackets" href="#id9" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, as
well as providing the option to use the weighted version used in Wang and
Simoncelli, 2008 <a class="footnote-reference brackets" href="#id11" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> (which was shown to consistently improve the image quality
prediction on the LIVE database). More info can be found online <a class="footnote-reference brackets" href="#id10" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<p>Note that this is a similarity metric (not a distance), and so 1 means the
two images are identical and 0 means they’re very different. When the two
images are negatively correlated, SSIM can be negative. SSIM is bounded
between -1 and 1.</p>
<p>This function returns the mean SSIM, a scalar-valued metric giving the
average over the whole image. For the SSIM map (showing the computed value
across the image), call <a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim_map" title="plenoptic.metric.perceptual_distance.ssim_map"><code class="xref py py-func docutils literal notranslate"><span class="pre">ssim_map</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>img1</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The first image or batch of images, of shape (batch, channel, height, width).</p></li>
<li><p><strong>img2</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The second image or batch of images, of shape (batch, channel, height, width).
The heights and widths of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> must be the same. The numbers of
batches and channels of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> need to be broadcastable: either
they are the same or one of them is 1. The output will be computed separately
for each channel (so channels are treated in the same way as batches). Both
images should have values between 0 and 1. Otherwise, the result may be
inaccurate, and we will raise a warning (but will still compute it).</p></li>
<li><p><strong>weighted</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span> (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)) – Whether to use the original, unweighted SSIM version (<code class="docutils literal notranslate"><span class="pre">False</span></code>) as used
in <a class="footnote-reference brackets" href="#id8" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> or the weighted version (<code class="docutils literal notranslate"><span class="pre">True</span></code>) as used in <a class="footnote-reference brackets" href="#id11" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>. See Notes
section for the weight.</p></li>
<li><p><strong>pad</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(in Python v3.14)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code></a>[<code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">'constant'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>]</span> (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)) – If not <code class="docutils literal notranslate"><span class="pre">False</span></code>, how to pad the image for the convolutions computing the
local average of each image. See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad</span></code></a> for how
these work.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>mssim</em> – 2d tensor of shape (batch, channel) containing the mean SSIM for each
image, averaged over the whole image.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> is not 4d.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different height or width.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different batch or channel, unless one of them has
    a 1 there, so they can be broadcast.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different dtypes.</p></li>
</ul>
</dd>
<dt class="field-odd">Warns<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>UserWarning</strong> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has multiple channels, as SSIM was designed for
grayscale images.</p></li>
<li><p><strong>UserWarning</strong> – If at least one scale from either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has height or width of
less than 11, since SSIM uses an 11x11 convolutional kernel.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The weight used when <code class="docutils literal notranslate"><span class="pre">weighted=True</span></code> is:</p>
<div class="math notranslate nohighlight">
\[\log((1+\frac{\sigma_1^2}{C_2})(1+\frac{\sigma_2^2}{C_2}))\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_2^2\)</span> are the variances of <code class="docutils literal notranslate"><span class="pre">img1</span></code>
and <code class="docutils literal notranslate"><span class="pre">img2</span></code>, respectively, and <span class="math notranslate nohighlight">\(C_2\)</span> is a constant. See <a class="footnote-reference brackets" href="#id11" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> for more
details.</p>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: From error measurement to structural similarity”
IEEE Transactions on Image Processing, vol. 13, no. 1, Jan. 2004.</p>
</aside>
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>matlab code: <a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/ssim_index.m">https://www.cns.nyu.edu/~lcv/ssim/ssim_index.m</a></p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>project page: <a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/">https://www.cns.nyu.edu/~lcv/ssim/</a></p>
</aside>
<aside class="footnote brackets" id="id11" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id6">2</a>,<a role="doc-backlink" href="#id7">3</a>)</span>
<p>Wang, Z., &amp; Simoncelli, E. P. (2008). Maximum differentiation (MAD)
competition: A methodology for comparing computational models of
perceptual discriminability. Journal of Vision, 8(12), 1–13.
<a class="reference external" href="https://dx.doi.org/10.1167/8.12.8">https://dx.doi.org/10.1167/8.12.8</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">einstein</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">ssim</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">img</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
<span class="go">tensor([[0.0519]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plenoptic.metric.perceptual_distance.ssim_map">
<span class="sig-name descname"><span class="pre">ssim_map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/plenoptic/metric/perceptual_distance.html#ssim_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plenoptic.metric.perceptual_distance.ssim_map" title="Link to this definition"></a></dt>
<dd><p>Structural similarity index map.</p>
<p>As described in Wang et al., 2004 <a class="footnote-reference brackets" href="#id16" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>, the structural similarity index (SSIM) is a
perceptual distance metric, giving the distance between two images. SSIM is based on
three comparison measurements between the two images: luminance, contrast, and
structure. All of these are computed convolutionally across the images. See the
references for more information.</p>
<p>This implementation follows the original implementation, as found online <a class="footnote-reference brackets" href="#id17" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>, as
well as providing the option to use the weighted version used in Wang and
Simoncelli, 2008 <a class="footnote-reference brackets" href="#id21" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> (which was shown to consistently improve the image quality
prediction on the LIVE database). More info can be found online <a class="footnote-reference brackets" href="#id19" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>.</p>
<p>Note that this is a similarity metric (not a distance), and so 1 means the
two images are identical and 0 means they’re very different. When the two
images are negatively correlated, SSIM can be negative. SSIM is bounded
between -1 and 1.</p>
<p>This function returns the SSIM map, showing the SSIM values across the
image. For the mean SSIM (a single value metric), call <a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim" title="plenoptic.metric.perceptual_distance.ssim"><code class="xref py py-func docutils literal notranslate"><span class="pre">ssim</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>img1</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The first image or batch of images, of shape (batch, channel, height, width).</p></li>
<li><p><strong>img2</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The second image or batch of images, of shape (batch, channel, height, width).
The heights and widths of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> must be the same. The numbers of
batches and channels of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> need to be broadcastable: either
they are the same or one of them is 1. The output will be computed separately
for each channel (so channels are treated in the same way as batches). Both
images should have values between 0 and 1. Otherwise, the result may be
inaccurate, and we will raise a warning (but will still compute it).</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>ssim_map</em> – 4d tensor containing the map of SSIM values.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> is not 4d.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different height or width.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different batch or channel, unless one of them has
    a 1 there, so they can be broadcast.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different dtypes.</p></li>
</ul>
</dd>
<dt class="field-odd">Warns<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>UserWarning</strong> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has multiple channels, as SSIM was designed for
grayscale images.</p></li>
<li><p><strong>UserWarning</strong> – If at least one scale from either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has height or width of
less than 11, since SSIM uses an 11x11 convolutional kernel.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">5</a><span class="fn-bracket">]</span></span>
<p>Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: From error measurement to structural similarity”
IEEE Transactions on Image Processing, vol. 13, no. 1, Jan. 2004.</p>
</aside>
<aside class="footnote brackets" id="id17" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">6</a><span class="fn-bracket">]</span></span>
<p>matlab code <a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/ssim_index.m">https://www.cns.nyu.edu/~lcv/ssim/ssim_index.m</a></p>
</aside>
<aside class="footnote brackets" id="id19" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">7</a><span class="fn-bracket">]</span></span>
<p>project page <a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/">https://www.cns.nyu.edu/~lcv/ssim/</a></p>
</aside>
<aside class="footnote brackets" id="id21" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">8</a><span class="fn-bracket">]</span></span>
<p>Wang, Z., &amp; Simoncelli, E. P. (2008). Maximum differentiation (MAD)
competition: A methodology for comparing computational models of
perceptual discriminability. Journal of Vision, 8(12), 1–13.
<a class="reference external" href="https://dx.doi.org/10.1167/8.12.8">https://dx.doi.org/10.1167/8.12.8</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">einstein</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ssim_map</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">ssim_map</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">img</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ssim_map</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([1, 1, 246, 246])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plenoptic.metric.perceptual_distance.ms_ssim">
<span class="sig-name descname"><span class="pre">ms_ssim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_factors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/plenoptic/metric/perceptual_distance.html#ms_ssim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plenoptic.metric.perceptual_distance.ms_ssim" title="Link to this definition"></a></dt>
<dd><p>Multiscale structural similarity index (MS-SSIM).</p>
<p>As described in Wang et al., 2003 <a class="footnote-reference brackets" href="#id24" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>, multiscale structural similarity index
(MS-SSIM) is an improvement upon structural similarity index (SSIM) that takes into
account the perceptual distance between two images on different scales.</p>
<p>SSIM is based on three comparison measurements between the two images:
luminance, contrast, and structure. All of these are computed convolutionally
across the images, producing three maps instead of scalars. The SSIM map is
the elementwise product of these three maps. See <a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim" title="plenoptic.metric.perceptual_distance.ssim"><code class="xref py py-func docutils literal notranslate"><span class="pre">ssim</span></code></a> and
<a class="reference internal" href="#plenoptic.metric.perceptual_distance.ssim_map" title="plenoptic.metric.perceptual_distance.ssim_map"><code class="xref py py-func docutils literal notranslate"><span class="pre">ssim_map</span></code></a> for a full description of SSIM.</p>
<p>To get images of different scales, average pooling operations with kernel
size 2 are performed recursively on the input images. The product of
contrast map and structure map (the “contrast-structure map”) is computed
for all but the coarsest scales, and the overall SSIM map is only computed
for the coarsest scale. Their mean values are raised to exponents and
multiplied to produce MS-SSIM:</p>
<div class="math notranslate nohighlight">
\[MSSSIM = {SSIM}_M^{a_M} \prod_{i=1}^{M-1} ({CS}_i)^{a_i}\]</div>
<p>Here <span class="math notranslate nohighlight">\(M\)</span> is the number of scales, <span class="math notranslate nohighlight">\({CS}_i\)</span> is the mean value
of the contrast-structure map for the i’th finest scale, and <span class="math notranslate nohighlight">\({SSIM}_M\)</span>
is the mean value of the SSIM map for the coarsest scale. If at least one
of these terms are negative, the value of MS-SSIM is zero. The values of
<span class="math notranslate nohighlight">\(a_i, i=1,...,M\)</span> are taken from the argument <code class="docutils literal notranslate"><span class="pre">power_factors</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>img1</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The first image or batch of images, of shape (batch, channel, height, width).</p></li>
<li><p><strong>img2</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The second image or batch of images, of shape (batch, channel, height, width).
The heights and widths of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> must be the same. The numbers of
batches and channels of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> need to be broadcastable: either
they are the same or one of them is 1. The output will be computed separately
for each channel (so channels are treated in the same way as batches). Both
images should have values between 0 and 1. Otherwise, the result may be
inaccurate, and we will raise a warning (but will still compute it).</p></li>
<li><p><strong>power_factors</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> | <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></span> (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)) – Power exponents for the mean values of maps, for different scales (from
fine to coarse). The length of this array determines the number of scales.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, set to <code class="docutils literal notranslate"><span class="pre">[0.0448,</span> <span class="pre">0.2856,</span> <span class="pre">0.3001,</span> <span class="pre">0.2363,</span> <span class="pre">0.1333]</span></code>, which is what
psychophysical experiments in Wang et al., 2003 <a class="footnote-reference brackets" href="#id24" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> found.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>msssim</em> – 2d tensor of shape (batch, channel) containing the MS-SSIM for each image.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> is not 4d.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different height or width.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different batch or channel, unless one of them has
    a 1 there, so they can be broadcast.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different dtypes.</p></li>
</ul>
</dd>
<dt class="field-odd">Warns<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>UserWarning</strong> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has multiple channels, as MS-SSIM was designed
for grayscale images.</p></li>
<li><p><strong>UserWarning</strong> – If at least one scale from either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has height or width of
less than 11, since SSIM uses an 11x11 convolutional kernel.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id24" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id22">1</a>,<a role="doc-backlink" href="#id23">2</a>)</span>
<p>Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. “Multiscale
structural similarity for image quality assessment.” The Thrity-Seventh
Asilomar Conference on Signals, Systems &amp; Computers, 2003. Vol. 2. IEEE, 2003.</p>
</aside>
</aside>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">einstein</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">ms_ssim</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">img</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
<span class="go">tensor([[0.4684]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plenoptic.metric.perceptual_distance.normalized_laplacian_pyramid">
<span class="sig-name descname"><span class="pre">normalized_laplacian_pyramid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/plenoptic/metric/perceptual_distance.html#normalized_laplacian_pyramid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plenoptic.metric.perceptual_distance.normalized_laplacian_pyramid" title="Link to this definition"></a></dt>
<dd><p>Compute the normalized Laplacian Pyramid using pre-optimized parameters.</p>
<p>Model parameters are those used in Laparra et al., 2016 <a class="footnote-reference brackets" href="#id27" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>, copied from the
matlab code used in the paper, found online <a class="footnote-reference brackets" href="#id28" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>img</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – Image, or batch of images of shape (batch, channel, height, width). This
representation is designed for grayscale images and will be computed separately
for each channel (so channels are treated in the same way as batches).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a>[<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]</span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>normalized_laplacian_activations</em> – The normalized Laplacian Pyramid with six scales.</p>
</dd>
</dl>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id27" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">10</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Ballé, J., Berardino, A. and Simoncelli, E.P., 2016. Perceptual
image quality assessment using a normalized Laplacian pyramid. Electronic
Imaging, 2016(16), pp.1-6.</p>
</aside>
<aside class="footnote brackets" id="id28" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">11</a><span class="fn-bracket">]</span></span>
<p>matlab code: <a class="reference external" href="https://www.cns.nyu.edu/~lcv/NLPyr/NLP_dist.m">https://www.cns.nyu.edu/~lcv/NLPyr/NLP_dist.m</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">einstein</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyramid</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">normalized_laplacian_pyramid</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pyramid</span><span class="p">]</span>
<span class="go">[torch.Size([1, 1, 256, 256]),</span>
<span class="go"> torch.Size([1, 1, 128, 128]),</span>
<span class="go"> torch.Size([1, 1, 64, 64]),</span>
<span class="go"> torch.Size([1, 1, 32, 32]),</span>
<span class="go"> torch.Size([1, 1, 16, 16]),</span>
<span class="go"> torch.Size([1, 1, 8, 8])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pyramid</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">&lt;PyrFigure size ...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference download internal" download="" href="../_downloads/54e53db8b577624c5ecac8a9dd0fea17/plenoptic-metric-perceptual_distance-1.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/f26f0100ecd3fe05cb48711660f7f2c9/plenoptic-metric-perceptual_distance-1.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/3afafa6de9a1dbebef7a73838ef4b262/plenoptic-metric-perceptual_distance-1.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="../_images/plenoptic-metric-perceptual_distance-1.png" class="plot-directive" src="../_images/plenoptic-metric-perceptual_distance-1.png" />
</figure>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plenoptic.metric.perceptual_distance.nlpd">
<span class="sig-name descname"><span class="pre">nlpd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/plenoptic/metric/perceptual_distance.html#nlpd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plenoptic.metric.perceptual_distance.nlpd" title="Link to this definition"></a></dt>
<dd><p>Compute the normalized Laplacian Pyramid Distance.</p>
<p>As described in Laparra et al., 2016 <a class="footnote-reference brackets" href="#id31" id="id29" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>, this is an image quality metric based on
the transformations associated with the early visual system: local luminance
subtraction and local contrast gain control.</p>
<p>A laplacian pyramid subtracts a local estimate of the mean luminance at six scales.
Then a local gain control divides these centered coefficients by a weighted sum of
absolute values in spatial neighborhood.</p>
<p>These weights parameters were optimized for redundancy reduction over an training
database of (undistorted) natural images, as described in the paper. Parameters were
copied from matlab code used for the paper, found online <a class="footnote-reference brackets" href="#id32" id="id30" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>.</p>
<p>Note that we compute root mean squared error for each scale, and then average over
these, effectively giving larger weight to the lower frequency coefficients
(which are fewer in number, due to subsampling).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>img1</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The first image or batch of images of shape (batch, channel, height, width).</p></li>
<li><p><strong>img2</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – The second image or batch of images of shape (batch, channel, height, width).
The heights and widths of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> must be the same. The numbers of
batches and channels of <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> need to be broadcastable: either
they are the same or one of them is 1. The output will be computed separately
for each channel (so channels are treated in the same way as batches). Both
images should have values between 0 and 1. Otherwise, the result may be
inaccurate, and we will raise a warning (but will still compute it).</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>distance</em> – The normalized Laplacian Pyramid distance, with shape (batch, channel).</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> is not 4d.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different height or width.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different batch or channel, unless one of them has
    a 1 there, so they can be broadcast.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">img1</span></code> and <code class="docutils literal notranslate"><span class="pre">img2</span></code> have different dtypes.</p></li>
</ul>
</dd>
<dt class="field-odd">Warns<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>UserWarning</strong> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has multiple channels, as SSIM was designed for
grayscale images.</p></li>
<li><p><strong>UserWarning</strong> – If either <code class="docutils literal notranslate"><span class="pre">img1</span></code> or <code class="docutils literal notranslate"><span class="pre">img2</span></code> has a value outside of range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id31" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">12</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Ballé, J., Berardino, A. and Simoncelli, E.P., 2016. Perceptual
image quality assessment using a normalized Laplacian pyramid. Electronic
Imaging, 2016(16), pp.1-6.</p>
</aside>
<aside class="footnote brackets" id="id32" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">13</a><span class="fn-bracket">]</span></span>
<p>matlab code: <a class="reference external" href="https://www.cns.nyu.edu/~lcv/NLPyr/NLP_dist.m">https://www.cns.nyu.edu/~lcv/NLPyr/NLP_dist.m</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">plenoptic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">po</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">einstein_img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">einstein</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">curie_img</span> <span class="o">=</span> <span class="n">po</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">curie</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">po</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">nlpd</span><span class="p">(</span><span class="n">einstein_img</span><span class="p">,</span> <span class="n">curie_img</span><span class="p">)</span>
<span class="go">tensor([[1.3507]])</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2025, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>