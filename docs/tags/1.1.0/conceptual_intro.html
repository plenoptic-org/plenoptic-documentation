

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Conceptual Introduction &mdash; plenoptic 1.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/tabs.css?v=4c969af8" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=fc837d61"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/tabs.js?v=3ee01567"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model requirements" href="models.html" />
    <link rel="prev" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Conceptual Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/conceptual_intro.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="conceptual-introduction">
<span id="conceptual-intro"></span><h1>Conceptual Introduction<a class="headerlink" href="#conceptual-introduction" title="Link to this heading"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is a python library for “model-based synthesis of perceptual
stimuli”. If you’ve never heard this phrase before, it may seem mysterious: what
is stimulus synthesis and what types of scientific investigation does it
facilitate?</p>
<p>Synthesis is a framework for exploring models by using them to create new
stimuli, rather than examining their responses to existing ones. <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code>
focuses on models of visual <a class="footnote-reference brackets" href="#id7" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> information processing, which take an image as
input, perform some computations based on parameters, and return some
vector-valued abstract representation as output. This output can be mapped to
neuronal firing rate, fMRI BOLD response, behavior on some task, image category,
etc., depending on the researchers’ intended question.</p>
<figure class="align-default" id="id9" style="width: 100%">
<span id="synthesis-schematic"></span><img alt="Schematic describing relationship between simulate, fit, and synthesize." src="_images/model_sim-fit-infer.svg" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Schematic describing relationship between simulate, fit, and synthesize.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>That is, computational models transform a stimulus <span class="math notranslate nohighlight">\(s\)</span> to a response
<span class="math notranslate nohighlight">\(r\)</span> (we often refer to <span class="math notranslate nohighlight">\(r\)</span> as “the model’s representation of
<span class="math notranslate nohighlight">\(s\)</span>”), based on some model parameters <span class="math notranslate nohighlight">\(\theta\)</span>. For example, a
trained neural network that classifies images has specific weights
<span class="math notranslate nohighlight">\(\theta\)</span>, accepts an image <span class="math notranslate nohighlight">\(s\)</span> and returns a one-hot vector
<span class="math notranslate nohighlight">\(r\)</span> that specifies the image class. Another example is a linear-nonlinear
oriented filter model of a simple cell in primary visual cortex, where
<span class="math notranslate nohighlight">\(\theta\)</span> defines the filter’s orientation, size, and spatial frequency,
the model accepts an image <span class="math notranslate nohighlight">\(s\)</span> and returns a scalar <span class="math notranslate nohighlight">\(r\)</span> that
represents the neuron’s firing rate.</p>
<p>The most common scientific uses for a model are to simulate responses or to fit
parameters, as illustrated in <a class="reference internal" href="#synthesis-schematic"><span class="std std-numref">Fig. 1</span></a>. For simulation, we hold
the parameters constant while presenting the model with inputs (e.g, photographs of dogs,
or a set of sine-wave gratings) and we run the
model to compute responses. For fitting, we use optimization to find the
parameter values that best account for the observed responses to a set of training
stimuli. In both of these cases, we are holding two of the three variables (<span class="math notranslate nohighlight">\(r\)</span>, <span class="math notranslate nohighlight">\(s\)</span>,
<span class="math notranslate nohighlight">\(\theta\)</span>) constant while computing or estimating the third. We can do the same thing to
generate novel stimuli, <span class="math notranslate nohighlight">\(s\)</span>, while holding the parameters and responses
constant. We refer to this process as <strong>synthesis</strong> and it facilitates the
exploration of input space to improve our understanding of a model’s
representations.</p>
<p>This is related to a long and fruitful thread of research in vision science that
focuses on what humans cannot see, that is, the information they are insensitive
to. Perceptual metamers — images that are physically distinct but perceptually
indistinguishable — provide direct evidence of such information loss in visual
representations. Color metamers were instrumental in the development of the
Young-Helmholtz theory of trichromacy <a class="reference internal" href="#helmholtz1852" id="id2"><span>[Helmholtz1852]</span></a>. In this context,
metamers demonstrate that the human visual system projects the infinite
dimensionality of the physical signal to three dimensions.</p>
<p>To make this more concrete, let’s walk through an example. Humans can see
visible light, which is electromagnetic radiation with wavelengths between 400
and 700 nanometers (nm). We often want to be able to recreate the colors in a
natural scene, such as when we take a picture. In order to do so, we can ask:
what information do we need to record in order to do so? Let’s start with a
solid patch of uniform color. If we wanted to recreate the complete energy
spectra of the color, we would need to record a lot of numbers: even if we
subsampled the wavelengths so that we only recorded the energy every 5 nm, we
would need 61 numbers per color! But we know that most modern electronic screens
only use three numbers, often called RGB (red, green, and blue) — why can we
get away with throwing away so much information? Trichromacy and color metamers
can help explain.</p>
<p>Researchers studying color perception arrived at a standard procedure – the bipartite color-matching experiment – for
constraining a model for trichromatic metamers, illustrated in <a class="reference internal" href="#trichromacy"><span class="std std-numref">Fig. 2</span></a>. An observer matches a monochromatic test
color (i.e., a light with energy at only a single wavelength) with the physical
mixture of three different monochromatic stimuli, called <strong>primaries</strong>. Thus,
the goal is to create two perceptually-indistinguishable stimuli (<strong>metamers</strong>).
Perhaps surprisingly, not only is this possible for any test
color, it is also possible for just about any selection of primaries (as long as they’re within the
visible light spectrum and sufficiently different from each other). For most human observers, three
primaries are required: there are many colors that cannot be matched with only two primaries, and four yields non-unique responses.
However, there are some people, for whom two primaries are sufficient.</p>
<figure class="align-default" id="id10" style="width: 100%">
<span id="trichromacy"></span><img alt="Color matching experiment." src="_images/trichromacy.svg" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Color matching experiment</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Requiring three primaries for most people, but two for some provided a hint regarding the underlying mechanisms:
most people have cone photorecpetors from three distinct classes (generally
referred to as S, M, and L, for “short”, “medium”, and “long”).  But some forms of color blindness arise from genetic
deviations in which only two classes are represented. Color metamers are created when cone
responses have been matched. Human cones transform colors from a
high-dimensional space (i.e., a vector describing the energy at each wavelength)
to a three-dimensional one (i.e., a vector describing how active each cone class
is). This means a large amount of wavelength information is discarded.</p>
<p>A worked example may help demonstrate this point more clearly. Let’s match the
random light shown on the left below using the primaries shown on the right.</p>
<p id="primaries">(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/03178b6f543a04b2cdde69b579a94252/conceptual_intro_primaries.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/b726dd1deab991956e8f94af39366874/conceptual_intro_primaries.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/4bc027cfe8443a2720d072e12b44146d/conceptual_intro_primaries.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default" id="id11">
<img alt="_images/conceptual_intro_primaries.png" class="plot-directive" src="_images/conceptual_intro_primaries.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Left: Random light whose appearance we will match. Right: primaries.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The only way we can change the matching light is multiply those primaries by
different numbers, moving them up and down. You might look at them and wonder
how we can match the light shown on the left, with all its random wiggles. The
important point is that we <strong>will not</strong> match those wiggles. We will instead
match the cone activation levels, which we get by matrix multiplying our light
by the cone fundamentals, shown below.</p>
<p>(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/1c8eab99f9dd6396277d0358a222da7a/conceptual_intro_cones.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/37e7919d138a2e2c608fb9e66823e1ea/conceptual_intro_cones.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/febea97ed019274548f2a31fbc3c8e94/conceptual_intro_cones.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default" id="id13">
<img alt="_images/conceptual_intro_cones.png" class="plot-directive" src="_images/conceptual_intro_cones.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Left: the cone sensitivity curves. Right: the response of each cone class to
the random light shown in <a class="reference internal" href="#primaries"><span class="std std-ref">the previous figure</span></a>.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>With some linear algebra, we can compute another light that has very different
amounts of energy at each wavelength but identical cone responses, shown below.</p>
<p>(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/2674bcfeef3e87c3bcb2e04f744f78b7/conceptual_intro_matched_light.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/25e965d06cde0e560dc10a17c5225acf/conceptual_intro_matched_light.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/3aff9c480cac3a08db7a31fcd2798f4f/conceptual_intro_matched_light.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="_images/conceptual_intro_matched_light.png" class="plot-directive" src="_images/conceptual_intro_matched_light.png" />
</figure>
<p>If we look at the plot on the left, we can see that the two lights are very
different physically, but we can see on the right that they generate the same
cone responses and thus would be perceived identically.</p>
<p>In this example, the model was a simple linear system of cone responses, and
thus we can generate a metamer, a physically different input with identical
output, via some simple linear algebra. Metamers can be useful for understanding
other systems as well, because discarding information is useful: the human
visual system is discarding information at every stage of processing, not just
at the cones’ absorption of light, and any computational system that seeks to
classify images must discard a lot of information about unnecessary differences
between images in the same class. However, generating metamer for other systems
gets complicated: when a system gets more complex, linear algebra no longer
suffices.</p>
<p>Let’s consider a slightly more complex example. Human vision is very finely
detailed at the center of gaze, but gradually discards this detailed spatial
information as distance to the center of gaze increases. This phenomenon is
known as <strong>foveation</strong>, and can be easily seen by the difficulty in reading a
paragraph of text or recognizing a face out of the corner of your eye (see
<a class="reference internal" href="#lettvin1976" id="id3"><span>[Lettvin1976]</span></a> for an accessible discussion with examples). The simplest
possible model of foveation would be to average pixel intensities in windows
whose width grows linearly with distance from the center of an image, as shown
in <a class="reference internal" href="#model-schematic"><span class="std std-numref">Fig. 7</span></a>:</p>
<figure class="align-default" id="id15" style="width: 100%">
<span id="model-schematic"></span><img alt="Foveated pixel intensity model." src="_images/model_schematic.svg" />
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">The foveated pixel intensity model averages pixel values in elliptical windows that grow in size as you move away from the center of the image. It only cares about the average in these regions, not the fine details.</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>This model cares about the average pixel intensity in a given area, but doesn’t
care how that average is reached. If the pixels in one of the ellipses above all
have a value of 0.5, if they’re half 0s and half 1s, if they’re randomly
distributed around 0.5 — those are all identical, as far as the model is
concerned. A more concrete example is shown in <a class="reference internal" href="#fov-met"><span class="std std-numref">Fig. 8</span></a>:</p>
<figure class="align-default" id="id16" style="width: 100%">
<span id="fov-met"></span><img alt="Three images, all identical as far as the foveated pixel intensity model is concerned." src="_images/foveated_mets.svg" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Three images that the foveated pixel intensity model considers identical. They all have the same average pixel values within the foveated elliptical regions (the red ellipse shows an example averaging region at that location), but differ greatly in their fine details.</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>These three images are all identical for the foveated pixel intensity model
described above (the red ellipse shows the size of the averaging region at that
location). These three images all have identical average pixel intensities in
small regions whose size grows as they move away from the center of the image.
However, like the color metamers discussed earlier, they are all very physically
different: the leftmost image is a natural image, the rightmost one has lots of
high-frequency noise, while the center one looks somewhat blurry. You might
think that, because the model only cares about average pixel intensities, you
can throw away all the fine details and the model won’t notice. And you can! But
you can also add whatever kind of fine details you’d like, including random
noise — the model is completely insensitive to them.</p>
<p>With relatively simple linear models like human trichromacy and the foveated
pixel intensity model, this way of thinking about models may seem unnecessary.
But it is very difficult to understand how models will perform on unexpected or
out-of-distribution data! The burgeoning literature on adversarial examples and
robustness in machine learning provides many of examples of this, such as the
addition of a small amount of noise (invisible to humans) changing the predicted
category <a class="reference internal" href="#szegedy2013" id="id4"><span>[Szegedy2013]</span></a> or the addition of a small elephant to a picture
completely changing detected objects’ identities and boundaries
<a class="reference internal" href="#rosenfeld2018" id="id5"><span>[Rosenfeld2018]</span></a>. Exploring model behavior on <em>all</em> possible inputs is
impossible — the space of all possible images is far too vast — but image
synthesis provides one mechanism for exploration in a targeted manner.</p>
<p>Furthermore, image synthesis provides a complementary method of comparing models
to the standard procedure. Generally, scientific models are evaluated on their
ability to fit data or perform a task, such as how well a model performs on
ImageNet or how closely a model tracks firing rate in some collected data.
However, many models can perform a task equally or comparably well <a class="footnote-reference brackets" href="#id8" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. By
using image synthesis to explore models’ representational spaces, we can gain a
fuller understanding of how models succeed and how they fail to capture the
phenomena under study.</p>
<section id="beyond-metamers">
<h2>Beyond Metamers<a class="headerlink" href="#beyond-metamers" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> contains more than just metamers — it provides a set of methods
for performing image synthesis. Each method allows for different exploration of
a model’s representational space:</p>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/intro/06_Metamer.html"><span class="doc">Metamers</span></a> investigate what features the model
disregards entirely.</p></li>
<li><p><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html"><span class="doc">Eigendistortions</span></a> investigates which
features the model considers the least and which it considers the most
important</p></li>
<li><p><a class="reference external" href="tutorials/intro/07_MAD_Competition.nblink">Maximal differentiation (MAD) competition</a> enables efficient comparison of two
metrics, highlighting the aspects in which their sensitivities differ.</p></li>
</ul>
<p>The goal of this package is to facilitate model exploration and understanding.
We hope that providing these tools helps tighten the model-experiment loop: when
a model is proposed, whether by importing from a related field or
earlier experiments, <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> enables scientists to make targeted
exploration of the model’s representational space, generating stimuli that will
provide the most information. We hope to help theorists become more active
participants in directing future experiments by efficiently finding new
predictions to test.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>These methods also work with auditory models, such as in <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html">Feather et al.,
2019</a>
though we haven’t yet implemented examples. If you’re interested, please
post in <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions)">Discussions</a>!</p>
</aside>
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>for example, as of February 2022, more than 100 models have above 95% top
5 accuracy on ImageNet, with 9 models within a percent of the top performer at
99.02%. Furthermore, the state of the art top 5 accuracy has been at or above
95% since 2016, with an improvement of only 4% in the past six years.</p>
</aside>
</aside>
<div role="list" class="citation-list">
<div class="citation" id="helmholtz1852" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Helmholtz1852</a><span class="fn-bracket">]</span></span>
<p>Helmholtz, H. (1852). LXXXI. on the theory of compound
colours. The London, Edinburgh, and Dublin Philosophical Magazine and Journal
of Science, 4(28), 519–534. <a class="reference external" href="https://dx.doi.org/10.1080/14786445208647175">https://dx.doi.org/10.1080/14786445208647175</a></p>
</div>
<div class="citation" id="lettvin1976" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Lettvin1976</a><span class="fn-bracket">]</span></span>
<p>Lettvin, J. Y. (1976). On Seeing Sidelong. The Sciences, 16(4),
10–20.
<a class="reference external" href="https://web.archive.org/web/20221016220044/https://jerome.lettvin.com/jerome/OnSeeingSidelong.pdf">https://web.archive.org/web/20221016220044/https://jerome.lettvin.com/jerome/OnSeeingSidelong.pdf</a></p>
</div>
<div class="citation" id="szegedy2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Szegedy2013</a><span class="fn-bracket">]</span></span>
<p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,
Goodfellow, I., &amp; Fergus, R. (2013). Intriguing properties of neural
networks. <a class="reference external" href="https://arxiv.org/abs/1312.6199">https://arxiv.org/abs/1312.6199</a></p>
</div>
<div class="citation" id="rosenfeld2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Rosenfeld2018</a><span class="fn-bracket">]</span></span>
<p>Rosenfeld, A., Zemel, R., &amp; Tsotsos, J.~K. (2018). The
elephant in the room. <a class="reference external" href="https://arxiv.org/abs/1808.03305">https://arxiv.org/abs/1808.03305</a></p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="models.html" class="btn btn-neutral float-right" title="Model requirements" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>