

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Conceptual Introduction &mdash; plenoptic 1.3.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=1f29e9d3"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=35a8b989"></script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model requirements" href="models.html" />
    <link rel="prev" title="Using Jupyter to Run Example Notebooks" href="jupyter.html" /> 
</head>

<body class="wy-body-for-nav">

<div style="background-color: rgb(248, 215, 218); color: rgb(114, 28, 36); text-align: center;">
  <div>
    <div>This is documentation for <strong>an old version</strong>.
      <a href="https://docs.plenoptic.org/" style="background-color: rgb(220, 53, 69); color: rgb(255, 255, 255); margin: 1rem; padding: 0.375rem 0.75rem; border-radius: 4px; display: inline-block; text-align: center;">Switch to stable version</a>
    </div>
  </div>
</div>
 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide and Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Conceptual Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/conceptual_intro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="conceptual-introduction">
<span id="conceptual-intro"></span><h1>Conceptual Introduction<a class="headerlink" href="#conceptual-introduction" title="Link to this heading"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is a python library for “model-based synthesis of perceptual stimuli”. If you’ve never heard this phrase before, it may seem mysterious: what is stimulus synthesis and what types of scientific investigation does it facilitate?</p>
<p>Synthesis is a framework for exploring models by using them to create new stimuli, rather than examining their responses to existing ones. <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> focuses on models of visual <a class="footnote-reference brackets" href="#footnote-1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> information processing, which take an image as input, perform some computations based on parameters, and return some vector-valued abstract representation as output. This output can be mapped to neuronal firing rate, fMRI BOLD response, behavior on some task, image category, etc., depending on the researchers’ intended question.</p>
<figure class="align-default" id="id7" style="width: 100%">
<span id="synthesis-schematic"></span><img alt="Schematic describing relationship between simulate, fit, and synthesize." src="_images/model_sim-fit-infer.svg" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Schematic describing relationship between simulate, fit, and synthesize.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>That is, computational models transform a stimulus <span class="math notranslate nohighlight">\(s\)</span> to a response <span class="math notranslate nohighlight">\(r\)</span> (we often refer to <span class="math notranslate nohighlight">\(r\)</span> as “the model’s representation of <span class="math notranslate nohighlight">\(s\)</span>”), based on some model parameters <span class="math notranslate nohighlight">\(\theta\)</span>. For example, a trained neural network that classifies images has specific weights <span class="math notranslate nohighlight">\(\theta\)</span>, accepts an image <span class="math notranslate nohighlight">\(s\)</span> and returns a one-hot vector <span class="math notranslate nohighlight">\(r\)</span> that specifies the image class. Another example is a linear-nonlinear oriented filter model of a simple cell in primary visual cortex, where <span class="math notranslate nohighlight">\(\theta\)</span> defines the filter’s orientation, size, and spatial frequency, the model accepts an image <span class="math notranslate nohighlight">\(s\)</span> and returns a scalar <span class="math notranslate nohighlight">\(r\)</span> that represents the neuron’s firing rate.</p>
<p>The most common scientific uses for a model are to simulate responses or to fit parameters, as illustrated in <a class="reference internal" href="#synthesis-schematic"><span class="std std-numref">Fig. 1</span></a>. For simulation, we hold the parameters constant while presenting the model with inputs (e.g, photographs of dogs, or a set of sine-wave gratings) and we run the model to compute responses. For fitting, we use optimization to find the parameter values that best account for the observed responses to a set of training stimuli. In both of these cases, we are holding two of the three variables (<span class="math notranslate nohighlight">\(r\)</span>, <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(\theta\)</span>) constant while computing or estimating the third. We can do the same thing to generate novel stimuli, <span class="math notranslate nohighlight">\(s\)</span>, while holding the parameters and responses constant. We refer to this process as <strong>synthesis</strong> and it facilitates the exploration of input space to improve our understanding of a model’s representations.</p>
<p>This is related to a long and fruitful thread of research in vision science that focuses on what humans cannot see, that is, the information they are insensitive to. Perceptual metamers — images that are physically distinct but perceptually indistinguishable — provide direct evidence of such information loss in visual representations. Color metamers were instrumental in the development of the Young-Helmholtz theory of trichromacy <span id="id2"><a class="reference internal" href="citation.html#id21" title="H. Helmholtz. Lxxxi. on the theory of compound colours. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 4(28):519–534, 1852. URL: https://doi.org/10.1080/14786445208647175, doi:10.1080/14786445208647175.">Helmholtz, 1852</a></span>. In this context, metamers demonstrate that the human visual system projects the infinite dimensionality of the physical signal to three dimensions.</p>
<p>To make this more concrete, let’s walk through an example. Humans can see visible light, which is electromagnetic radiation with wavelengths between 400 and 700 nanometers (nm). We often want to be able to recreate the colors in a natural scene, such as when we take a picture. In order to do so, we can ask: what information do we need to record in order to do so? Let’s start with a solid patch of uniform color. If we wanted to recreate the complete energy spectra of the color, we would need to record a lot of numbers: even if we subsampled the wavelengths so that we only recorded the energy every 5 nm, we would need 61 numbers per color! But we know that most modern electronic screens only use three numbers, often called RGB (red, green, and blue) — why can we get away with throwing away so much information? Trichromacy and color metamers can help explain.</p>
<p>Researchers studying color perception arrived at a standard procedure – the bipartite color-matching experiment – for constraining a model for trichromatic metamers, illustrated in <a class="reference internal" href="#trichromacy"><span class="std std-numref">Fig. 2</span></a>. An observer matches a monochromatic test color (i.e., a light with energy at only a single wavelength) with the physical mixture of three different monochromatic stimuli, called <strong>primaries</strong>. Thus, the goal is to create two perceptually-indistinguishable stimuli (<strong>metamers</strong>). Perhaps surprisingly, not only is this possible for any test color, it is also possible for just about any selection of primaries (as long as they’re within the visible light spectrum and sufficiently different from each other). For most human observers, three primaries are required: there are many colors that cannot be matched with only two primaries, and four yields non-unique responses. However, there are some people, for whom two primaries are sufficient.</p>
<figure class="align-default" id="id8" style="width: 100%">
<span id="trichromacy"></span><img alt="Color matching experiment." src="_images/trichromacy.svg" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Color matching experiment</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Requiring three primaries for most people, but two for some provided a hint regarding the underlying mechanisms: most people have cone photorecpetors from three distinct classes (generally referred to as S, M, and L, for “short”, “medium”, and “long”). But some forms of color blindness arise from genetic deviations in which only two classes are represented. Color metamers are created when cone responses have been matched. Human cones transform colors from a high-dimensional space (i.e., a vector describing the energy at each wavelength) to a three-dimensional one (i.e., a vector describing how active each cone class is). This means a large amount of wavelength information is discarded.</p>
<p>A worked example may help demonstrate this point more clearly. Let’s match the random light shown on the left below using the primaries shown on the right.</p>
<p id="primaries">(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/03178b6f543a04b2cdde69b579a94252/conceptual_intro_primaries.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/b726dd1deab991956e8f94af39366874/conceptual_intro_primaries.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/4bc027cfe8443a2720d072e12b44146d/conceptual_intro_primaries.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default" id="id9">
<img alt="_images/conceptual_intro_primaries.png" class="plot-directive" src="_images/conceptual_intro_primaries.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Left: Random light whose appearance we will match. Right: primaries.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The only way we can change the matching light is multiply those primaries by different numbers, moving them up and down. You might look at them and wonder how we can match the light shown on the left, with all its random wiggles. The important point is that we <strong>will not</strong> match those wiggles. We will instead match the cone activation levels, which we get by matrix multiplying our light by the cone fundamentals, shown below.</p>
<p>(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/1c8eab99f9dd6396277d0358a222da7a/conceptual_intro_cones.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/37e7919d138a2e2c608fb9e66823e1ea/conceptual_intro_cones.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/febea97ed019274548f2a31fbc3c8e94/conceptual_intro_cones.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default" id="id11">
<img alt="_images/conceptual_intro_cones.png" class="plot-directive" src="_images/conceptual_intro_cones.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Left: the cone sensitivity curves. Right: the response of each cone class to
the random light shown in <a class="reference internal" href="#primaries"><span class="std std-ref">the previous figure</span></a>.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>With some linear algebra, we can compute another light that has very different amounts of energy at each wavelength but identical cone responses, shown below.</p>
<p>(<a class="reference download internal" download="" href="_downloads/1529dad3c6f78dd530d43172bfef8f56/conceptual_intro.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="_downloads/2674bcfeef3e87c3bcb2e04f744f78b7/conceptual_intro_matched_light.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="_downloads/25e965d06cde0e560dc10a17c5225acf/conceptual_intro_matched_light.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="_downloads/3aff9c480cac3a08db7a31fcd2798f4f/conceptual_intro_matched_light.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="_images/conceptual_intro_matched_light.png" class="plot-directive" src="_images/conceptual_intro_matched_light.png" />
</figure>
<p>If we look at the plot on the left, we can see that the two lights are very different physically, but we can see on the right that they generate the same cone responses and thus would be perceived identically.</p>
<p>In this example, the model was a simple linear system of cone responses, and thus we can generate a metamer, a physically different input with identical output, via some simple linear algebra. Metamers can be useful for understanding other systems as well, because discarding information is useful: the human visual system is discarding information at every stage of processing, not just at the cones’ absorption of light, and any computational system that seeks to classify images must discard a lot of information about unnecessary differences between images in the same class. However, generating metamer for other systems gets complicated: when a system gets more complex, linear algebra no longer suffices.</p>
<p>Let’s consider a slightly more complex example. Human vision is very finely detailed at the center of gaze, but gradually discards this detailed spatial information as distance to the center of gaze increases. This phenomenon is known as <strong>foveation</strong>, and can be easily seen by the difficulty in reading a paragraph of text or recognizing a face out of the corner of your eye (see <span id="id3"><a class="reference internal" href="citation.html#id24" title="Jerome Y. Lettvin. On seeing sidelong. The Sciences, 16(4):10–20, jul 1976. URL: https://doi.org/10.1002%2Fj.2326-1951.1976.tb01231.x, doi:10.1002/j.2326-1951.1976.tb01231.x.">Lettvin, 1976</a></span> for an accessible discussion with examples). The simplest possible model of foveation would be to average pixel intensities in windows whose width grows linearly with distance from the center of an image, as shown in <a class="reference internal" href="#model-schematic"><span class="std std-numref">Fig. 7</span></a>:</p>
<figure class="align-default" id="id13" style="width: 100%">
<span id="model-schematic"></span><img alt="Foveated pixel intensity model." src="_images/model_schematic.svg" />
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">The foveated pixel intensity model averages pixel values in elliptical windows that grow in size as you move away from the center of the image. It only cares about the average in these regions, not the fine details.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>This model cares about the average pixel intensity in a given area, but doesn’t care how that average is reached. If the pixels in one of the ellipses above all have a value of 0.5, if they’re half 0s and half 1s, if they’re randomly distributed around 0.5 — those are all identical, as far as the model is concerned. A more concrete example is shown in <a class="reference internal" href="#fov-met"><span class="std std-numref">Fig. 8</span></a>:</p>
<figure class="align-default" id="id14" style="width: 100%">
<span id="fov-met"></span><img alt="Three images, all identical as far as the foveated pixel intensity model is concerned." src="_images/foveated_mets.svg" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Three images that the foveated pixel intensity model considers identical. They all have the same average pixel values within the foveated elliptical regions (the red ellipse shows an example averaging region at that location), but differ greatly in their fine details.</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>These three images are all identical for the foveated pixel intensity model described above (the red ellipse shows the size of the averaging region at that location). These three images all have identical average pixel intensities in small regions whose size grows as they move away from the center of the image. However, like the color metamers discussed earlier, they are all very physically different: the leftmost image is a natural image, the rightmost one has lots of high-frequency noise, while the center one looks somewhat blurry. You might think that, because the model only cares about average pixel intensities, you can throw away all the fine details and the model won’t notice. And you can! But you can also add whatever kind of fine details you’d like, including random noise — the model is completely insensitive to them.</p>
<p>With relatively simple linear models like human trichromacy and the foveated pixel intensity model, this way of thinking about models may seem unnecessary. But it is very difficult to understand how models will perform on unexpected or out-of-distribution data! The burgeoning literature on adversarial examples and robustness in machine learning provides many of examples of this, such as the addition of a small amount of noise (invisible to humans) changing the predicted category <span id="id4">[<a class="reference internal" href="citation.html#id29" title="Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. ArXiv e-prints, pages 1–10, dec 2013. arXiv:1312.6199.">Szegedy <em>et al.</em>, 2013</a>]</span> or the addition of a small elephant to a picture completely changing detected objects’ identities and boundaries <span id="id5">[<a class="reference internal" href="citation.html#id26" title="A. Rosenfeld, R. Zemel, and J. K. Tsotsos. The Elephant in the Room. ArXiv e-prints, aug 2018. arXiv:1808.03305.">Rosenfeld <em>et al.</em>, 2018</a>]</span>. Exploring model behavior on <em>all</em> possible inputs is impossible — the space of all possible images is far too vast — but image synthesis provides one mechanism for exploration in a targeted manner.</p>
<p>Furthermore, image synthesis provides a complementary method of comparing models to the standard procedure. Generally, scientific models are evaluated on their ability to fit data or perform a task, such as how well a model performs on ImageNet or how closely a model tracks firing rate in some collected data. However, many models can perform a task equally or comparably well <a class="footnote-reference brackets" href="#footnote-2" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. By using image synthesis to explore models’ representational spaces, we can gain a fuller understanding of how models succeed and how they fail to capture the phenomena under study.</p>
<section id="beyond-metamers">
<h2>Beyond Metamers<a class="headerlink" href="#beyond-metamers" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> contains more than just metamers — it provides a set of methods for performing image synthesis. Each method allows for different exploration of a model’s representational space:</p>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/intro/06_Metamer.html"><span class="std std-doc">Metamers</span></a> investigate what features the model disregards entirely.</p></li>
<li><p><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html"><span class="std std-doc">Eigendistortions</span></a> investigates which features the model considers the least and which it considers the most important</p></li>
<li><p><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html"><span class="std std-doc">Maximal differentiation (MAD) competition</span></a> enables efficient comparison of two metrics, highlighting the aspects in which their sensitivities differ.</p></li>
</ul>
<p>The goal of this package is to facilitate model exploration and understanding. We hope that providing these tools helps tighten the model-experiment loop: when a model is proposed, whether by importing from a related field or earlier experiments, <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> enables scientists to make targeted exploration of the model’s representational space, generating stimuli that will provide the most information. We hope to help theorists become more active participants in directing future experiments by efficiently finding new predictions to test.</p>
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnote-1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>These methods also work with auditory models, such as in <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html">Feather et al.,
2019</a>
though we haven’t yet implemented examples. If you’re interested, please
post in <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions)">Discussions</a>!</p>
</aside>
<aside class="footnote brackets" id="footnote-2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>for example, as of February 2022, more than 100 models have above 95% top
5 accuracy on ImageNet, with 9 models within a percent of the top performer at
99.02%. Furthermore, the state of the art top 5 accuracy has been at or above
95% since 2016, with an improvement of only 4% in the past six years.</p>
</aside>
</aside>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="jupyter.html" class="btn btn-neutral float-left" title="Using Jupyter to Run Example Notebooks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="models.html" class="btn btn-neutral float-right" title="Model requirements" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>