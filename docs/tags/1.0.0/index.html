

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>plenoptic &mdash; plenoptic 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8d563738"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/05_Geodesics.html">Representational Geodesic</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">plenoptic</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="plenoptic">
<h1>plenoptic<a class="headerlink" href="#plenoptic" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://pypi.org/project/plenoptic/"><img alt="pypi-shield" src="https://img.shields.io/pypi/v/plenoptic.svg" /></a> <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/blob/main/LICENSE"><img alt="license-shield" src="https://img.shields.io/badge/license-MIT-yellow.svg" /></a> <img alt="python-version-shield" src="https://img.shields.io/badge/python-3.7%7C3.8%7C3.9%7C3.10-blue.svg" /> <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/actions?query=workflow%3Abuild"><img alt="build" src="https://github.com/LabForComputationalVision/plenoptic/workflows/build/badge.svg" /></a> <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/actions?query=workflow%3Atutorials"><img alt="tutorials" src="https://github.com/LabForComputationalVision/plenoptic/workflows/tutorials/badge.svg" /></a> <a class="reference external" href="https://doi.org/10.5281/zenodo.3995057"><img alt="zenodo" src="https://zenodo.org/badge/DOI/10.5281/zenodo.3995057.svg" /></a> <a class="reference external" href="https://mybinder.org/v2/gh/LabForComputationalVision/plenoptic/1.0.0?filepath=examples"><img alt="binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
<img alt="plenoptic logo" class="align-center" src="_images/plenoptic_logo_wide.svg" />
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is a python library for model-based stimulus synthesis. It
provides tools to help researchers understand their model by synthesizing novel
informative stimuli, which help build intuition for what features the model
ignores and what it is sensitive to. These synthetic images can then be used in
future perceptual or neural experiments for further investigation.</p>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>If you are unfamiliar with stimulus synthesis, see the <a class="reference internal" href="conceptual_intro.html#conceptual-intro"><span class="std std-ref">Conceptual Introduction</span></a>
for an in-depth introduction.</p></li>
<li><p>If you understand the basics of synthesis and want to get started using
<code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> quickly, see the <a class="reference internal" href="tutorials/00_quickstart.html"><span class="doc">Quickstart</span></a>
tutorial.</p></li>
</ul>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h3>
<p>The best way to install <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is via <code class="docutils literal notranslate"><span class="pre">pip</span></code>.</p>
<p>$ pip install plenoptic</p>
<p>See the <a class="reference internal" href="install.html#install"><span class="std std-ref">Installation</span></a> page for more details, including how to set up an isolated
virtual environment (recommended).</p>
</section>
<section id="ffmpeg-and-videos">
<h3>ffmpeg and videos<a class="headerlink" href="#ffmpeg-and-videos" title="Link to this heading"></a></h3>
<p>Several methods in this package generate videos. There are several backends
possible for saving the animations to file, see <a class="reference external" href="https://matplotlib.org/stable/api/animation_api.html#writer-classes">matplotlib documentation</a> for more
details. In order convert them to HTML5 for viewing (and thus, to view in a
jupyter notebook), you’ll need <a class="reference external" href="https://ffmpeg.org/download.html">ffmpeg</a>
installed and on your path as well. Depending on your system, this might already
be installed.</p>
<p>To change the backend, run <code class="docutils literal notranslate"><span class="pre">matplotlib.rcParams['animation.writer']</span> <span class="pre">=</span> <span class="pre">writer</span></code>
before calling any of the animate functions. If you try to set that <code class="docutils literal notranslate"><span class="pre">rcParam</span></code>
with a random string, <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> will tell you the available choices.</p>
</section>
</section>
<section id="contents">
<span id="package-contents"></span><h2>Contents<a class="headerlink" href="#contents" title="Link to this heading"></a></h2>
<section id="synthesis-methods">
<h3>Synthesis methods<a class="headerlink" href="#synthesis-methods" title="Link to this heading"></a></h3>
<figure class="align-default" style="width: 100%">
<img alt="The four synthesis methods included in plenoptic" src="_images/example_synth.svg" />
</figure>
<ul class="simple">
<li><p><a class="reference external" href="tutorials/06_Metamer.nblink">Metamers</a>: given a model and a reference image,
stochastically generate a new image whose model representation is identical to
that of the reference image. This method investigates what image features the
model disregards entirely.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#portilla2000" id="id1"><span>[Portilla2000]</span></a>, <a class="reference internal" href="#freeman2011" id="id2"><span>[Freeman2011]</span></a>, <a class="reference internal" href="#deza2019" id="id3"><span>[Deza2019]</span></a>,
<a class="reference internal" href="#feather2019" id="id4"><span>[Feather2019]</span></a>, <a class="reference internal" href="#wallis2019" id="id5"><span>[Wallis2019]</span></a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="tutorials/02_Eigendistortions.nblink">Eigendistortions</a>: given a model and a
reference image, compute the image perturbation that produces the smallest and
largest changes in the model response space. This method investigates the
image features the model considers the least and most important.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#berardino2017" id="id6"><span>[Berardino2017]</span></a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="tutorials/07_MAD_Competition.nblink">Maximal differentiation (MAD) competition</a>: given two metrics that measure distance
between images and a reference image, generate pairs of images that optimally
differentiate the models. Specifically, synthesize a pair of images that the
first model says are equi-distant from the reference while the second model
says they are maximally/minimally distant from the reference. Then synthesize
a second pair with the roles of the two models reversed. This method allows
for efficient comparison of two metrics, highlighting the aspects in which
their sensitivities differ.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#wang2008" id="id7"><span>[Wang2008]</span></a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="tutorials/05_Geodesics.nblink">Geodesics</a>: given a model and two images,
synthesize a sequence of images that lie on the shortest (“geodesic”) path in
the model’s representation space. This method investigates how a model
represents motion and what changes to an image it consider reasonable.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#henaff2016" id="id8"><span>[Henaff2016]</span></a>, <a class="reference internal" href="#henaff2020" id="id9"><span>[Henaff2020]</span></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="models-metrics-and-model-components">
<h3>Models, Metrics, and Model Components<a class="headerlink" href="#models-metrics-and-model-components" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Portilla-Simoncelli texture model, <a class="reference internal" href="#portilla2000" id="id10"><span>[Portilla2000]</span></a>, which measures the
statistical properties of visual textures, here defined as “repeating visual
patterns.”</p></li>
<li><p>Steerable pyramid, <a class="reference internal" href="#simoncelli1995" id="id11"><span>[Simoncelli1995]</span></a>, a multi-scale oriented image
decomposition. The basis are oriented (steerable) filters, localized in space
and frequency. Among other uses, the steerable pyramid serves as a good
representation from which to build a primary visual cortex model. See the
<a class="reference external" href="https://pyrtools.readthedocs.io/en/latest/index.html">pyrtools documentation</a> for more details on
image pyramids in general and the steerable pyramid in particular.</p></li>
<li><p>Structural Similarity Index (SSIM), <a class="reference internal" href="#wang2004" id="id12"><span>[Wang2004]</span></a>, is a perceptual similarity
metric, returning a number between -1 (totally different) and 1 (identical)
reflecting how similar two images are. This is based on the images’ luminance,
contrast, and structure, which are computed convolutionally across the images.</p></li>
<li><p>Multiscale Structrual Similarity Index (MS-SSIM), <a class="reference internal" href="#wang2003" id="id13"><span>[Wang2003]</span></a>, is a perceptual
similarity metric similar to SSIM, except it operates at multiple scales
(i.e., spatial frequencies).</p></li>
<li><p>Normalized Laplacian distance, <a class="reference internal" href="#laparra2016" id="id14"><span>[Laparra2016]</span></a> and <a class="reference internal" href="#laparra2017" id="id15"><span>[Laparra2017]</span></a>, is a
perceptual distance metric based on transformations associated with the early
visual system: local luminance subtraction and local contrast gain control, at
six scales.</p></li>
</ul>
</section>
</section>
<section id="getting-help">
<h2>Getting help<a class="headerlink" href="#getting-help" title="Link to this heading"></a></h2>
<p>We communicate via several channels on Github:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/discussions">Discussions</a> is the
place to ask usage questions, discuss issues too broad for a single issue, or
show off what you’ve made with plenoptic.</p></li>
<li><p>If you’ve come across a bug, open an <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/issues">issue</a>.</p></li>
<li><p>If you have an idea for an extension or enhancement, please post in the <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/discussions/categories/ideas">ideas
section</a>
of discussions first. We’ll discuss it there and, if we decide to pursue it,
open an issue to track progress.</p></li>
<li><p>See the <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/blob/main/CONTRIBUTING.md">contributing guide</a>
for how to get involved.</p></li>
</ul>
<p>In all cases, please follow our <a class="reference external" href="https://github.com/LabForComputationalVision/plenoptic/blob/main/CODE_OF_CONDUCT.md">code of conduct</a>.</p>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/05_Geodesics.html">Representational Geodesic</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>
</div>
<div role="list" class="citation-list">
<div class="citation" id="portilla2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Portilla2000<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Portilla, J., &amp; Simoncelli, E. P. (2000). A parametric texture
model based on joint statistics of complex wavelet coefficients.
International journal of computer vision, 40(1), 49–70.
<a class="reference external" href="https://www.cns.nyu.edu/~lcv/texture/">https://www.cns.nyu.edu/~lcv/texture/</a>.
<a class="reference external" href="https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf">https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf</a></p>
</div>
<div class="citation" id="freeman2011" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Freeman2011</a><span class="fn-bracket">]</span></span>
<p>Freeman, J., &amp; Simoncelli, E. P. (2011). Metamers of the
ventral stream. Nature Neuroscience, 14(9), 1195–1201.
<a class="reference external" href="http://www.cns.nyu.edu/pub/eero/freeman10-reprint.pdf">http://www.cns.nyu.edu/pub/eero/freeman10-reprint.pdf</a></p>
</div>
<div class="citation" id="deza2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Deza2019</a><span class="fn-bracket">]</span></span>
<p>Deza, A., Jonnalagadda, A., &amp; Eckstein, M. P. (2019). Towards
metamerism via foveated style transfer. In , International Conference on
Learning Representations.</p>
</div>
<div class="citation" id="feather2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Feather2019</a><span class="fn-bracket">]</span></span>
<p>Feather, J., Durango, A., Gonzalez, R., &amp; McDermott, J. (2019).
Metamers of neural networks reveal divergence from human perceptual systems.
In NeurIPS (pp. 10078–10089).</p>
</div>
<div class="citation" id="wallis2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Wallis2019</a><span class="fn-bracket">]</span></span>
<p>Wallis, T. S., Funke, C. M., Ecker, A. S., Gatys, L. A.,
Wichmann, F. A., &amp; Bethge, M. (2019). Image content is more important than
bouma’s law for scene metamers. eLife. <a class="reference external" href="http://dx.doi.org/10.7554/elife.42512">http://dx.doi.org/10.7554/elife.42512</a></p>
</div>
<div class="citation" id="berardino2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Berardino2017</a><span class="fn-bracket">]</span></span>
<p>Berardino, A., Laparra, V., J Ball'e, &amp; Simoncelli, E. P.
(2017). Eigen-distortions of hierarchical representations. In I. Guyon, U.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett,
Adv. Neural Information Processing Systems (NIPS*17) (pp. 1–10). : Curran
Associates, Inc. <a class="reference external" href="https://www.cns.nyu.edu/~lcv/eigendistortions/">https://www.cns.nyu.edu/~lcv/eigendistortions/</a>
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf">http://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf</a></p>
</div>
<div class="citation" id="wang2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Wang2008</a><span class="fn-bracket">]</span></span>
<p>Wang, Z., &amp; Simoncelli, E. P. (2008). Maximum differentiation
(MAD) competition: A methodology for comparing computational models of
perceptual discriminability. Journal of Vision, 8(12), 1–13.
<a class="reference external" href="https://ece.uwaterloo.ca/~z70wang/research/mad/">https://ece.uwaterloo.ca/~z70wang/research/mad/</a>
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/wang08-preprint.pdf">http://www.cns.nyu.edu/pub/lcv/wang08-preprint.pdf</a></p>
</div>
<div class="citation" id="henaff2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Henaff2016</a><span class="fn-bracket">]</span></span>
<p>H'enaff, O.~J., &amp; Simoncelli, E.~P. (2016). Geodesics of
learned representations. ICLR.
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/henaff16b-reprint.pdf">http://www.cns.nyu.edu/pub/lcv/henaff16b-reprint.pdf</a></p>
</div>
<div class="citation" id="henaff2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Henaff2020</a><span class="fn-bracket">]</span></span>
<p>O Hénaff, Y Bai, J Charlton, I Nauhaus, E P Simoncelli and R L T
Goris. Primary visual cortex straightens natural video trajectories Nature
Communications, vol.12(5982), Oct 2021.
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/henaff20-reprint.pdf">https://www.cns.nyu.edu/pub/lcv/henaff20-reprint.pdf</a></p>
</div>
<div class="citation" id="simoncelli1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Simoncelli1995</a><span class="fn-bracket">]</span></span>
<p>Simoncelli, E. P., &amp; Freeman, W. T. (1995). The steerable
pyramid: A flexible architecture for multi-scale derivative computation. In ,
Proc 2nd IEEE Int’l Conf on Image Proc (ICIP) (pp. 444–447). Washington, DC:
IEEE Sig Proc Society. <a class="reference external" href="http://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf">http://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf</a></p>
</div>
<div class="citation" id="wang2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">Wang2004</a><span class="fn-bracket">]</span></span>
<p>Wang, Z., Bovik, A., Sheikh, H., &amp; Simoncelli, E. (2004). Image
quality assessment: from error visibility to structural similarity. IEEE
Transactions on Image Processing, 13(4), 600–612.
<a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/">https://www.cns.nyu.edu/~lcv/ssim/</a>.
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf">http://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf</a></p>
</div>
<div class="citation" id="wang2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">Wang2003</a><span class="fn-bracket">]</span></span>
<p>Z Wang, E P Simoncelli and A C Bovik. Multiscale structural
similarity for image quality assessment Proc 37th Asilomar Conf on Signals,
Systems and Computers, vol.2 pp. 1398–1402, Nov 2003.
<a class="reference external" href="http://www.cns.nyu.edu/pub/eero/wang03b.pdf">http://www.cns.nyu.edu/pub/eero/wang03b.pdf</a></p>
</div>
<div class="citation" id="laparra2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">Laparra2017</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Berardino, A., Johannes Ball'e, &amp;
Simoncelli, E. P. (2017). Perceptually Optimized Image Rendering. Journal of
the Optical Society of America A, 34(9), 1511.
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/laparra17a.pdf">http://www.cns.nyu.edu/pub/lcv/laparra17a.pdf</a></p>
</div>
<div class="citation" id="laparra2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">Laparra2016</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Ballé, J., Berardino, A. and Simoncelli,
E.P., 2016. Perceptual image quality assessment using a normalized Laplacian
pyramid. Electronic Imaging, 2016(16), pp.1-6.
<a class="reference external" href="http://www.cns.nyu.edu/pub/lcv/laparra16a-reprint.pdf">http://www.cns.nyu.edu/pub/lcv/laparra16a-reprint.pdf</a></p>
</div>
</div>
<p>This package is supported by the Simons Foundation Flatiron Institute’s Center
for Computational Neuroscience.</p>
<img alt="Flatiron Institute Center for Computational Neuroscience logo" class="align-center" src="_images/CCN-logo-wText.png" />
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Lab for Computational Vision.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>