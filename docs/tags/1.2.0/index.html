

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>plenoptic &mdash; plenoptic 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/tabs.css?v=4c969af8" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=6efca38a"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/tabs.js?v=3ee01567"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav">

<div style="background-color: rgb(248, 215, 218); color: rgb(114, 28, 36); text-align: center;">
  <div>
    <div>This is documentation for <strong>an old version</strong>.
      <a href="https://docs.plenoptic.org/" style="background-color: rgb(220, 53, 69); color: rgb(255, 255, 255); margin: 1rem; padding: 0.375rem 0.75rem; border-radius: 4px; display: inline-block; text-align: center;">Switch to stable version</a>
    </div>
  </div>
</div>
 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            plenoptic
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html#ffmpeg-and-videos">ffmpeg and videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">plenoptic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">plenoptic</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="plenoptic">
<h1>plenoptic<a class="headerlink" href="#plenoptic" title="Link to this heading"></a></h1>
<p id="index"><a class="reference external" href="https://pypi.org/project/plenoptic/"><img alt="pypi-shield" src="https://img.shields.io/pypi/v/plenoptic.svg" /></a> <a class="reference external" href="https://anaconda.org/conda-forge/plenoptic"><img alt="conda-shield" src="https://anaconda.org/conda-forge/plenoptic/badges/version.svg" /></a> <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/LICENSE"><img alt="license-shield" src="https://img.shields.io/badge/license-MIT-yellow.svg" /></a> <img alt="python-version-shield" src="https://img.shields.io/badge/python-3.10%7C3.11%7C3.12-blue.svg" /> <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/actions?query=workflow%3Abuild"><img alt="build" src="https://github.com/plenoptic-org/plenoptic/workflows/build/badge.svg" /></a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.10151130"><img alt="zenodo" src="https://zenodo.org/badge/DOI/10.5281/zenodo.10151130.svg" /></a> <a class="reference external" href="https://codecov.io/gh/plenoptic-org/plenoptic"><img alt="codecov" src="https://codecov.io/gh/plenoptic-org/plenoptic/branch/main/graph/badge.svg?token=EDtl5kqXKA" /></a> <a class="reference external" href="https://mybinder.org/v2/gh/plenoptic-org/plenoptic/1.2.0?filepath=examples"><img alt="binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
<img alt="plenoptic logo" class="align-center" src="_images/plenoptic_logo_wide.svg" />
<p><code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is a python library for model-based synthesis of perceptual stimuli. For <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code>, models are those of visual <a class="footnote-reference brackets" href="#id20" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> information processing: they accept an image as input, perform some computations, and return some output, which can be mapped to neuronal firing rate, fMRI BOLD response, behavior on some task, image category, etc. The intended audience is researchers in neuroscience, psychology, and machine learning. The generated stimuli enable interpretation of model properties through examination of features that are enhanced, suppressed, or discarded. More importantly, they can facilitate the scientific process, through use in further perceptual or neural experiments aimed at validating or falsifying model predictions.</p>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>If you are unfamiliar with stimulus synthesis, see the <a class="reference internal" href="conceptual_intro.html#conceptual-intro"><span class="std std-ref">Conceptual Introduction</span></a>
for an in-depth introduction.</p></li>
<li><p>Otherwise, see the <a class="reference internal" href="tutorials/00_quickstart.html"><span class="doc">Quickstart</span></a>
tutorial.</p></li>
</ul>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h3>
<p>The best way to install <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> is via <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install plenoptic
</pre></div>
</div>
<p>or <code class="docutils literal notranslate"><span class="pre">conda</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ conda install plenoptic -c conda-forge
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We do not currently support conda installs on Windows, due to the lack of a Windows pytorch package on conda-forge. See <a class="reference external" href="https://github.com/conda-forge/pytorch-cpu-feedstock/issues/32">here</a> for the status of that issue.</p>
</div>
<p>See the <a class="reference internal" href="install.html#install"><span class="std std-ref">Installation</span></a> page for more details, including how to set up an isolated
virtual environment (recommended).</p>
</section>
<section id="ffmpeg-and-videos">
<h3>ffmpeg and videos<a class="headerlink" href="#ffmpeg-and-videos" title="Link to this heading"></a></h3>
<p>Some methods in this package generate videos. There are several backends
available for saving the animations to file (see <a class="reference external" href="https://matplotlib.org/stable/api/animation_api.html#writer-classes">matplotlib documentation</a>
).
To convert them to HTML5 for viewing (for example, in a
jupyter notebook), you’ll need <a class="reference external" href="https://ffmpeg.org/download.html">ffmpeg</a>
installed. Depending on your system, this might already
be installed, but if not, the easiest way is probably through <a class="reference external" href="https://anaconda.org/conda-forge/ffmpeg">conda</a>: <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span>
<span class="pre">ffmpeg</span></code>.
To change the backend, run <code class="docutils literal notranslate"><span class="pre">matplotlib.rcParams['animation.writer']</span> <span class="pre">=</span> <span class="pre">writer</span></code>
before calling any of the animate functions. If you try to set that <code class="docutils literal notranslate"><span class="pre">rcParam</span></code>
with a random string, <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> will list the available choices.</p>
</section>
</section>
<section id="contents">
<span id="package-contents"></span><h2>Contents<a class="headerlink" href="#contents" title="Link to this heading"></a></h2>
<figure class="align-default" style="width: 100%">
<img alt="The four synthesis methods included in plenoptic" src="_images/example_synth.svg" />
</figure>
<section id="synthesis-methods">
<h3>Synthesis methods<a class="headerlink" href="#synthesis-methods" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/intro/06_Metamer.html"><span class="doc">Metamers</span></a>: given a model and a reference image,
stochastically generate a new image whose model representation is identical to
that of the reference image (a “metamer”, as originally defined in the literature on Trichromacy).
This method makes explicit those features that the model retains/discards.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#portilla2000" id="id2"><span>[Portilla2000]</span></a>, <a class="reference internal" href="#freeman2011" id="id3"><span>[Freeman2011]</span></a>, <a class="reference internal" href="#deza2019" id="id4"><span>[Deza2019]</span></a>,
<a class="reference internal" href="#feather2019" id="id5"><span>[Feather2019]</span></a>, <a class="reference internal" href="#wallis2019" id="id6"><span>[Wallis2019]</span></a>, <a class="reference internal" href="#ziemba2021" id="id7"><span>[Ziemba2021]</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html"><span class="doc">Eigendistortions</span></a>: given a model and a
reference image, compute the image perturbations that produce the smallest/largest
change in the model response space. These are the
image changes to which the model is least/most sensitive, respectively.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#berardino2017" id="id8"><span>[Berardino2017]</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html"><span class="doc">Maximal differentiation (MAD) competition</span></a>: given a reference image and two models that measure distance
between images, generate pairs of images that optimally
differentiate the models. Specifically, synthesize a pair of images that are equi-distant from
the reference image according to model-1, but maximally/minimally distant according to model-2.  Synthesize
a second pair with the roles of the two models reversed. This method allows
for efficient comparison of two metrics, highlighting the aspects in which
their sensitivities most differ.</p>
<ul>
<li><p>Example papers: <a class="reference internal" href="#wang2008" id="id9"><span>[Wang2008]</span></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="models-metrics-and-model-components">
<h3>Models, Metrics, and Model Components<a class="headerlink" href="#models-metrics-and-model-components" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Steerable pyramid, <a class="reference internal" href="#simoncelli1992" id="id10"><span>[Simoncelli1992]</span></a> and <a class="reference internal" href="#simoncelli1995" id="id11"><span>[Simoncelli1995]</span></a>, a multi-scale
oriented image decomposition. Images are decomposed with a family of oriented
filters, localized in space and frequency, similar to the “Gabor functions”
commonly used to model receptive fields in primary visual cortex. The critical
difference is that the pyramid organizes these filters so as to effeciently
cover the 4D space of (x,y) positions, orientations, and scales, enabling
efficient interpolation and interpretation (<a class="reference external" href="https://www.cns.nyu.edu/~eero/STEERPYR/">further info</a> ). See the <a class="reference external" href="https://pyrtools.readthedocs.io/en/latest/index.html">pyrtools documentation</a> for more details on
python tools for image pyramids in general and the steerable pyramid in
particular.</p></li>
<li><p>Portilla-Simoncelli texture model, <a class="reference internal" href="#portilla2000" id="id12"><span>[Portilla2000]</span></a>, which computes a set of image statistics
that capture the appearance of visual textures (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/texture/">further info</a>).</p></li>
<li><p>Structural Similarity Index (SSIM), <a class="reference internal" href="#wang2004" id="id14"><span>[Wang2004]</span></a>, is a perceptual similarity
metric, that takes two images and returns a value between -1 (totally different) and 1 (identical)
reflecting their similarity (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim">further info</a>).</p></li>
<li><p>Multiscale Structural Similarity Index (MS-SSIM), <a class="reference internal" href="#wang2003" id="id16"><span>[Wang2003]</span></a>, is an extension of SSIM
that operates jointly over multiple scales.</p></li>
<li><p>Normalized Laplacian distance, <a class="reference internal" href="#laparra2016" id="id17"><span>[Laparra2016]</span></a> and <a class="reference internal" href="#laparra2017" id="id18"><span>[Laparra2017]</span></a>, is a
perceptual distance metric based on transformations associated with the early
visual system: local luminance subtraction and local contrast gain control, at
six scales (<a class="reference external" href="https://www.cns.nyu.edu/~lcv/NLPyr/">further info</a>).</p></li>
</ul>
</section>
</section>
<section id="getting-help">
<h2>Getting help<a class="headerlink" href="#getting-help" title="Link to this heading"></a></h2>
<p>We communicate via several channels on Github:</p>
<ul class="simple">
<li><p>To report a bug, open an <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/issues">issue</a>.</p></li>
<li><p>To send suggestions for extensions or enhancements, please post in the <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions/categories/ideas">ideas
section</a>
of discussions first. We’ll discuss it there and, if we decide to pursue it,
open an issue to track progress.</p></li>
<li><p>To ask usage questions, discuss broad issues, or
show off what you’ve made with plenoptic, go to <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions">Discussions</a>.</p></li>
<li><p>To contribute to the project, see the <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/CONTRIBUTING.md">contributing guide</a>.</p></li>
</ul>
<p>In all cases, we request that you respect our <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/blob/main/CODE_OF_CONDUCT.md">code of conduct</a>.</p>
</section>
<section id="citing-us">
<h2>Citing us<a class="headerlink" href="#citing-us" title="Link to this heading"></a></h2>
<p>If you use <code class="docutils literal notranslate"><span class="pre">plenoptic</span></code> in a published academic article or presentation, please
cite us! See the <a class="reference internal" href="citation.html#citation"><span class="std std-ref">Citation Guide</span></a> for more details.</p>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">Using Jupyter to Run Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html#ffmpeg-and-videos">ffmpeg and videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="conceptual_intro.html">Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Model requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/00_quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation Guide</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method introductions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/02_Eigendistortions.html">Eigendistortions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/06_Metamer.html">Metamers</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/07_Simple_MAD.html">MAD Competition Conceptual Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/intro/08_MAD_Competition.html">MAD Competition Usage</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Models and metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/03_Steerable_Pyramid.html">Steerable Pyramid</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/04_Perceptual_distance.html">Perceptual distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/models/Metamer-Portilla-Simoncelli.html">Portilla-Simoncelli Texture Metamer</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Synthesis method examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/09_Original_MAD.html">Reproducing Wang and Simoncelli, 2008 (MAD Competition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/applications/Demo_Eigendistortion.html">Reproducing Berardino et al., 2017 (Eigendistortions)</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Advanced usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="synthesis.html">Synthesis object design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Display.html">Display and animate functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/advanced/Synthesis_extensions.html">Extending existing synthesis objects</a></li>
</ul>
</div>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id20" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>These methods also work with auditory models, such as in <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html">Feather et al.,
2019</a>
though we haven’t yet implemented examples. If you’re interested, please
post in <a class="reference external" href="https://github.com/plenoptic-org/plenoptic/discussions)">Discussions</a>!</p>
</aside>
</aside>
<div role="list" class="citation-list">
<div class="citation" id="portilla2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Portilla2000<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Portilla, J., &amp; Simoncelli, E. P. (2000). A parametric texture
model based on joint statistics of complex wavelet coefficients.
International journal of computer vision, 40(1), 49–70.
<a class="reference external" href="https://www.cns.nyu.edu/~lcv/texture/">https://www.cns.nyu.edu/~lcv/texture/</a>.
<a class="reference external" href="https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf">https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf</a></p>
</div>
<div class="citation" id="freeman2011" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Freeman2011</a><span class="fn-bracket">]</span></span>
<p>Freeman, J., &amp; Simoncelli, E. P. (2011). Metamers of the
ventral stream. Nature Neuroscience, 14(9), 1195–1201.
<a class="reference external" href="https://www.cns.nyu.edu/pub/eero/freeman10-reprint.pdf">https://www.cns.nyu.edu/pub/eero/freeman10-reprint.pdf</a></p>
</div>
<div class="citation" id="deza2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Deza2019</a><span class="fn-bracket">]</span></span>
<p>Deza, A., Jonnalagadda, A., &amp; Eckstein, M. P. (2019). Towards
metamerism via foveated style transfer. In , International Conference on
Learning Representations.</p>
</div>
<div class="citation" id="feather2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Feather2019</a><span class="fn-bracket">]</span></span>
<p>Feather, J., Durango, A., Gonzalez, R., &amp; McDermott, J. (2019).
Metamers of neural networks reveal divergence from human perceptual systems.
In NeurIPS (pp. 10078–10089).</p>
</div>
<div class="citation" id="wallis2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Wallis2019</a><span class="fn-bracket">]</span></span>
<p>Wallis, T. S., Funke, C. M., Ecker, A. S., Gatys, L. A.,
Wichmann, F. A., &amp; Bethge, M. (2019). Image content is more important than
bouma’s law for scene metamers. eLife. <a class="reference external" href="https://dx.doi.org/10.7554/elife.42512">https://dx.doi.org/10.7554/elife.42512</a></p>
</div>
<div class="citation" id="berardino2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Berardino2017</a><span class="fn-bracket">]</span></span>
<p>Berardino, A., Laparra, V., J Ball'e, &amp; Simoncelli, E. P.
(2017). Eigen-distortions of hierarchical representations. In I. Guyon, U.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett,
Adv. Neural Information Processing Systems (NIPS*17) (pp. 1–10). : Curran
Associates, Inc. <a class="reference external" href="https://www.cns.nyu.edu/~lcv/eigendistortions/">https://www.cns.nyu.edu/~lcv/eigendistortions/</a>
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf">https://www.cns.nyu.edu/pub/lcv/berardino17c-final.pdf</a></p>
</div>
<div class="citation" id="wang2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Wang2008</a><span class="fn-bracket">]</span></span>
<p>Wang, Z., &amp; Simoncelli, E. P. (2008). Maximum differentiation
(MAD) competition: A methodology for comparing computational models of
perceptual discriminability. Journal of Vision, 8(12), 1–13.
<a class="reference external" href="https://ece.uwaterloo.ca/~z70wang/research/mad/">https://ece.uwaterloo.ca/~z70wang/research/mad/</a>
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/wang08-preprint.pdf">https://www.cns.nyu.edu/pub/lcv/wang08-preprint.pdf</a></p>
</div>
<div class="citation" id="simoncelli1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Simoncelli1992</a><span class="fn-bracket">]</span></span>
<p>Simoncelli, E. P., Freeman, W. T., Adelson, E. H., &amp;
Heeger, D. J. (1992). Shiftable Multi-Scale Transforms. IEEE Trans.
Information Theory, 38(2), 587–607. <a class="reference external" href="https://dx.doi.org/10.1109/18.119725">https://dx.doi.org/10.1109/18.119725</a></p>
</div>
<div class="citation" id="simoncelli1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Simoncelli1995</a><span class="fn-bracket">]</span></span>
<p>Simoncelli, E. P., &amp; Freeman, W. T. (1995). The steerable
pyramid: A flexible architecture for multi-scale derivative computation. In ,
Proc 2nd IEEE Int’l Conf on Image Proc (ICIP) (pp. 444–447). Washington, DC:
IEEE Sig Proc Society. <a class="reference external" href="https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf">https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf</a></p>
</div>
<div class="citation" id="wang2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">Wang2004</a><span class="fn-bracket">]</span></span>
<p>Wang, Z., Bovik, A., Sheikh, H., &amp; Simoncelli, E. (2004). Image
quality assessment: from error visibility to structural similarity. IEEE
Transactions on Image Processing, 13(4), 600–612.
<a class="reference external" href="https://www.cns.nyu.edu/~lcv/ssim/">https://www.cns.nyu.edu/~lcv/ssim/</a>.
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf">https://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf</a></p>
</div>
<div class="citation" id="wang2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">Wang2003</a><span class="fn-bracket">]</span></span>
<p>Z Wang, E P Simoncelli and A C Bovik. Multiscale structural
similarity for image quality assessment Proc 37th Asilomar Conf on Signals,
Systems and Computers, vol.2 pp. 1398–1402, Nov 2003.
<a class="reference external" href="https://www.cns.nyu.edu/pub/eero/wang03b.pdf">https://www.cns.nyu.edu/pub/eero/wang03b.pdf</a></p>
</div>
<div class="citation" id="laparra2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">Laparra2017</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Berardino, A., Johannes Ball'e, &amp;
Simoncelli, E. P. (2017). Perceptually Optimized Image Rendering. Journal of
the Optical Society of America A, 34(9), 1511.
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/laparra17a.pdf">https://www.cns.nyu.edu/pub/lcv/laparra17a.pdf</a></p>
</div>
<div class="citation" id="laparra2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">Laparra2016</a><span class="fn-bracket">]</span></span>
<p>Laparra, V., Ballé, J., Berardino, A. and Simoncelli,
E.P., 2016. Perceptual image quality assessment using a normalized Laplacian
pyramid. Electronic Imaging, 2016(16), pp.1-6.
<a class="reference external" href="https://www.cns.nyu.edu/pub/lcv/laparra16a-reprint.pdf">https://www.cns.nyu.edu/pub/lcv/laparra16a-reprint.pdf</a></p>
</div>
<div class="citation" id="ziemba2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Ziemba2021</a><span class="fn-bracket">]</span></span>
<p>Ziemba, C.M., and Simoncelli, E.P. (2021). Opposing effects of selectivity and invariance in peripheral vision.
Nature Communications, vol.12(4597).
<a class="reference external" href="https://dx.doi.org/10.1038/s41467-021-24880-5">https://dx.doi.org/10.1038/s41467-021-24880-5</a></p>
</div>
</div>
<p>This package is supported by the <a class="reference external" href="https://www.simonsfoundation.org/flatiron/center-for-computational-neuroscience/">Center for Computational Neuroscience</a>,
in the Flatiron Institute of the Simons Foundation.</p>
<img alt="Flatiron Institute Center for Computational Neuroscience logo" class="align-center" src="_images/CCN-logo-wText.png" />
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Plenoptic authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>